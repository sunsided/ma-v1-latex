\documentclass[a4paper, 11pt, oneside, ngerman]{thesis}
% TODO: Check twoside book-like formatting and page numbers

\usepackage[ngerman]{babel}
\usepackage[autostyle,german=quotes]{csquotes}

% thesis class
\renewcommand\figureshortname{Abb.}
\renewcommand\tableshortname{Tab.}

\usepackage[mode=buildnew]{standalone}

%\usepackage[miktex]{gnuplottex}

\usepackage{fancyhdr}
\usepackage{eurosym}
\usepackage{geometry}
\usepackage{lipsum}
\usepackage{graphicx}
\usepackage{lmodern}
\usepackage{wrapfig}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{inconsolata}
\usepackage{xspace}

\usepackage[figurewithin=section, 
		   font=small, 
		   labelfont=bf]
		   {caption}
\usepackage{subcaption}

\usepackage[super]{nth}
\usepackage{makeidx}

\usepackage{amsmath}
\usepackage{bm}
\usepackage{isomath}
\usepackage{mathtools}
\usepackage{xfrac}
\usepackage{cancel}
\usepackage[binary-units=true]{siunitx}

\usepackage{booktabs}

\usepackage{xcolor}
\usepackage{pgfplots}
\pgfplotsset{compat=1.13}

\usepackage{tikz}
\usetikzlibrary{calc}
\usetikzlibrary{fit}

\usepackage[backend=biber,style=authoryear]{biblatex}

\usepackage{listings}

\usepackage{varioref}
\usepackage[pdftex]{hyperref}
\usepackage{cleveref}
\usepackage{todonotes}

% fancy headers
\newcommand{\changefont}{%
    \fontsize{9}{11}\selectfont
}

\fancyhead[LE]{\changefont \slshape \rightmark} %section
\fancyhead[RE]{\changefont \thepage}
\fancyhead[RO]{\changefont \slshape \leftmark} % chapter
\fancyhead[LO]{\changefont \thepage}

% color definitions for listings
\definecolor{bluekeywords}{rgb}{0.13,0.13,1}
\definecolor{greencomments}{rgb}{0,0.5,0}
\definecolor{redstrings}{rgb}{0.9,0,0}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\hypersetup{urlcolor=black, colorlinks=true}

\lstset{literate=
  {á}{{\'a}}1 {é}{{\'e}}1 {í}{{\'i}}1 {ó}{{\'o}}1 {ú}{{\'u}}1
  {Á}{{\'A}}1 {É}{{\'E}}1 {Í}{{\'I}}1 {Ó}{{\'O}}1 {Ú}{{\'U}}1
  {à}{{\`a}}1 {è}{{\`e}}1 {ì}{{\`i}}1 {ò}{{\`o}}1 {ù}{{\`u}}1
  {À}{{\`A}}1 {È}{{\'E}}1 {Ì}{{\`I}}1 {Ò}{{\`O}}1 {Ù}{{\`U}}1
  {ä}{{\"a}}1 {ë}{{\"e}}1 {ï}{{\"i}}1 {ö}{{\"o}}1 {ü}{{\"u}}1
  {Ä}{{\"A}}1 {Ë}{{\"E}}1 {Ï}{{\"I}}1 {Ö}{{\"O}}1 {Ü}{{\"U}}1
  {â}{{\^a}}1 {ê}{{\^e}}1 {î}{{\^i}}1 {ô}{{\^o}}1 {û}{{\^u}}1
  {Â}{{\^A}}1 {Ê}{{\^E}}1 {Î}{{\^I}}1 {Ô}{{\^O}}1 {Û}{{\^U}}1
  {œ}{{\oe}}1 {Œ}{{\OE}}1 {æ}{{\ae}}1 {Æ}{{\AE}}1 {ß}{{\ss}}1
  {ű}{{\H{u}}}1 {Ű}{{\H{U}}}1 {ő}{{\H{o}}}1 {Ő}{{\H{O}}}1
  {ç}{{\c c}}1 {Ç}{{\c C}}1 {ø}{{\o}}1 {å}{{\r a}}1 {Å}{{\r A}}1
  {€}{{\EUR}}1 {£}{{\pounds}}1
}

\newcommand{\listingpartial}{\mbox{$\partial$}}

%\lstdefinestyle{csharp}{
%	language=[Sharp]C,
%}

\lstdefinestyle{matlab}{
	language=matlab,
	keepspaces=true,
	columns=flexible,
	showspaces=false,
	showtabs=false,
	breaklines=true,
	showstringspaces=false,
	breakatwhitespace=true,
	escapeinside={/*@}{@*/},
	backgroundcolor=\color{backcolour},  
	commentstyle=\color{greencomments},
	keywordstyle=\color{bluekeywords}\bfseries,
	stringstyle=\color{redstrings},
	basicstyle=\footnotesize\ttfamily,
	columns=fullflexible,
	tabsize=2,
	numbers=left,                    
    numbersep=5pt,
    numberblanklines=true,
    morekeywords={syms,pretty}
}

% cleveref bindings for other packages
\crefname{lstlisting}{listing}{listings}
\Crefname{lstlisting}{Listing}{Listings}

\bibliography{quellen.bib}

\geometry{a4paper,
		top=25mm, 
		left=40mm, 
		right=25mm, 
		bottom=30mm, 
		headsep=10mm, 
		footskip=12mm}

\graphicspath{ {./images/}{./images/source/}{./images/sift-scales/} }

\pagestyle{plain}
\pagenumbering{arabic}

% Generate the index and glossary
\makeindex

\newcommand{\name}[1]{\textsc{#1}\xspace}
\newcommand{\algo}[1]{\textit{#1}\xspace}
\newcommand{\acro}[1]{\texttt{#1}\xspace}
\newcommand{\datatype}[1]{\texttt{#1}\xspace}
\newcommand{\imgfile}[1]{\texttt{#1}\xspace}
\newcommand{\engl}[1]{engl.: \textit{#1}\xspace}

\newcommand{\LAB}{L\textsuperscript{\tiny *}a\textsuperscript{\tiny *}b\textsuperscript{\tiny *}\index{Lab}\xspace}
\newcommand{\LUV}{L\textsuperscript{\tiny *}u\textsuperscript{\tiny *}v\textsuperscript{\tiny *}\index{Luv}\xspace}
\newcommand{\RGB}{RGB\index{RGB}\xspace}
\newcommand{\HLS}{HLS\index{HLS}\xspace}
\newcommand{\HSV}{HSV\index{HSV}\xspace}

\newcommand{\LoG}{LoG\index{Laplacian-of-Gaussian}\index{Gaussian!Laplacian-of-Gaussian}\xspace}
\newcommand{\DoG}{DoG\index{Difference-of-Gaussian}\index{Gaussian!Difference-of-Gaussian}\xspace}

\newcommand{\MATLAB}{MATLAB\index{MATLAB}\xspace}

% math commands
\renewcommand{\vec}{\vectorsym}
\newcommand{\mat}{\matrixsym}
\newcommand{\fun}{\mathit}
\newcommand{\transp}{^{\mathstrut\scriptscriptstyle{\top}}}
\DeclareMathOperator*{\argmin}{\arg\!\min}
\DeclareMathOperator*{\median}{median}
\newcommand{\thetavec}{\ensuremath{\vec{\theta}}}
\newcommand{\order}[1]{\ensuremath{\mathcal{O}(#1)}}
\newcommand{\mean}[1]{\ensuremath{\bar{#1}}}
\newcommand{\parens}[1]{\ensuremath{\left(#1\right)}}
\newcommand{\brackets}[1]{\ensuremath{\left[#1\right]}}
\newcommand{\braces}[1]{\ensuremath{\left\{#1\right\}}}
\newcommand{\myexp}[1]{\mathsf{e}^{#1}}

\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

% "SI" units
\sisetup{detect-weight=true, detect-family=true}
\DeclareSIUnit	\px{px}

\hypersetup{pdftitle={Large Scale Image Retrieval Using Features Extracted From Mobile Phone Camera Stills}}
\hypersetup{pdfsubject={Image Retrieval}} 
\hypersetup{pdfauthor={Markus Mayer}} 
\hypersetup{pdfkeywords={{Image Retrieval}, {CBIR}, {Image Matching}}}

\begin{titlepage}
	\title{\textbf{Large Scale Image Retrieval} \\ Using Features Extracted From \\ Mobile Phone Camera Stills}
	\author{Markus Mayer}
	\date{\today}
\end{titlepage}

\begin{document}

%Term definitions
%\newglossaryentry{sse}{name=SSE, description={Sum of Squared Errors}}

\maketitle

\begin{abstract}
Like regular \textit{Image Retrieval}, but on a larger scale.
\end{abstract}

\frontmatter
\pagenumbering{roman} 
\pagestyle{fancy}

\clearpage
\phantomsection
\addcontentsline{toc}{section}{Inhaltsverzeichnis}
\tableofcontents

\clearpage
\phantomsection
\addcontentsline{toc}{section}{Abbildungsverzeichnis}
\listoffigures

\clearpage
\phantomsection
\addcontentsline{toc}{section}{Tabellenverzeichnis}
\listoftables

\clearpage
\phantomsection
\addcontentsline{toc}{section}{Listings}
\lstlistoflistings

\mainmatter
\pagenumbering{arabic}
\pagestyle{fancy}

\chapter{Bildvorverarbeitung}

\section{Kanalreduktion und Graustufenumwandlung}

Während für einige Methoden der Bildanalyse und des Content-Based Image Retrieval Farbinformationen -- z.B. in Form von Farbhisto- und -korrelogrammen -- eine Rolle spielen, ist dies für andere Methoden -- etwa Scale-Invariant Feature Transforms oder Local Binary Patterns - nicht der Fall.
Eine Grundannahme ist hier, dass bildrelevante Merkmale weniger in Farb-, als in Intensitätsunterschieden zu finden sind, wie sie etwa durch Objektkanten hervorgerufen werden.
Die Transformation von unabhängigen Farbintensitätskanälen, wie sie aus der additiven (RGB-) oder subtraktiven (CMY-) Farbmischung bekannt sind, auf einen einzigen (idealerweise dekorrelierten) Intensitätskanal führt dabei ebenfalls zu einer Verringerung der notwendigen Verarbeitungszeit und Speicherkapazität, was insbesondere einer Echtzeit-Datenverarbeitung entgegenkommt.

\subsection{Methoden der Graustufenumwandlung}
\label{subsec:graustufen}

\cite{kanan2012color} weisen darauf hin, dass die exakte Methode der Umwandlung von Farb- in Intensitätswerte oft nicht näher spezifiziert und als irrelevant bezüglich der weiteren Verarbeitung angenommen wird.
Sie vergleichen daher verschiedene wahrnehmungs- und nicht-wahrnehmungspsychologisch inspirierte Methoden der Konvertierung und überprüfen diese mittels der Extraktion der Featuredeskriptoren \algo{SIFT}, \algo{SURF}, \algo{Geometric Blur} und \algo{Local Binary Patterns} an verschiedenen Bilddatensätzen, u.a. kategorisierten Objekten in Caltech 101 (\cite{fei2007learning,caltech101:online}), Gesichtern des AR Face Dataset (\cite{martinez1998benavente}) und Objekten in variablen Ausleuchtungsszenarien in \cite{barnard2002data}. 
Hervorzuheben sind hierbei die Methoden \algo{Intensity}, \algo{Gleam}, \algo{Luminance}, \algo{Luma} und \algo{Lightness}, sowie \algo{Luster}.

Der Wertebereich wird als $0 \dots 1$ angenommen, wobei einige der Methoden sich der Gamma-Korrekturfunktion
%
\begin{align}
\Gamma(t) = t^{\gamma}
\end{align}
%
mit $\gamma = \sfrac{1}{2.2}$ bedienen.
Diese Funktion überführt die lineare Farbintensität $t$ in eine nichtlineare, der menschlichen Helligkeitswahrnehmung ähnlicheren Abbildung. \todo{Quelle}
Hierbei hellen Werte $\gamma > 1$ das Bild auf (d.h. die Intensitäten werden verstärkt), während Werte $\gamma < 1$ das Bild abdunkeln (vgl. \cref{fig:gammaCorrections}).
Bei einem Bild $\mat I$ mit gleichverteilten Intensitätswerten (d.h. geringem Kontrast) führt eine Korrektur mit $\gamma = \sfrac{1}{2.2} \approx 0.45$ dabei zu einer mittleren Aufhellung um ca. $\num{19} \pm \SI{9}{\percent}$:
%
\begin{align}
E(\Gamma(\mat I) - \mat I) &= E(\mat I^{\sfrac{1}{2.2}} - \mat I) \approx 0.1866 \\
\sigma (\Gamma(\mat I) - \mat I) &= \sigma(\mat I^{\sfrac{1}{2.2}} - \mat I) \approx 0.0857
\end{align}
%
Zur schnelleren Verarbeitung von 8-bit-Bildern mit 256 Intensitätsabstufungen pro Kanal kann zur Vermeidung von Fließkommaoperationen eine Lookup-Tabelle verwendet werden, wie sie etwa gemäß \cref{lst:matlab_gamma} erstellt werden kann.

Die Methoden \algo{Intensity} und \algo{Gleam} entsprechen dem regulären und gamma-korrigierten Mittelwert der drei \RGB-Farbkanäle (vgl. \cref{fig:conv_intensity,fig:conv_gleam}):
%
\begin{align}
\mat I_{Intensity} &= \frac{1}{3}(\mat R + \mat G + \mat B) \label{eq:intensity} \\
\mat I_{Gleam} &= \frac{1}{3}(\Gamma(\mat R) + \Gamma(\mat G) + \Gamma(\mat B)) \label{eq:gleam}
\end{align}

"`\algo{Luster}"' ist der \textit{Lightness}-Kanal des \HLS-Farbraumes (Hue, Lightness, Saturation) und entspricht dem Mittelwert der minimalen und maximalen \RGB-Kanalintensitäten
%
\begin{align}
\mat I_{Luster} &= \frac{1}{2} \left( \min (\mat R, \mat G, \mat B) + \max (\mat R, \mat G, \mat B) \right)
\end{align}
%
und entspricht einer ausreißertoleranteren Variante des Value-Kanals $\mat I_{Value} = \max (\mat R, \mat G, \mat B)$ des \HSV-Farbraumes (Hue, Saturation, Value); vgl. \cref{fig:conv_luster,fig:conv_value}.

Die Methoden \algo{Luminance} \todo{Quelle Pratt} und \algo{Luma} (vgl. \cref{fig:conv_luminance,fig:conv_luma}) arbeiten als gewichtete Summe der einzelnen Farbintensitäten, definiert als
%
\begin{align}
\mat I_{Luminance} &= 0.299 \mat R + 0.587 \mat G + 0.114 \mat B \\
\mat I_{Luma} &= 0.2126 \, \Gamma(\mat R) + 0.7152 \, \Gamma(\mat G) + 0.0722 \, \Gamma(\mat B)
\end{align}
%
wobei $\mat I_{Luma}$ einer gammakorrigierten Form von $\mat I_{Luminance}$ entspricht, die für High-Definition-TV-Geräte entwickelt wurde \todo{Quelle}.
\Cite{nixon2012feature} definieren $\mat I_{Luminance}$ derweil unterschiedlich als
%
\begin{align*}
\mat I_{Luminance} &= 0.18 \mat R + 0.79 \mat G + 0.02 \mat B
\end{align*}
%
Dieser Unterschied dürfte jedoch keinen maßgeblichen Einfluss auf das von \citeauthor{kanan2012color} ermittelte Ergebnis haben.

\algo{Lightness} schlussendlich entspricht der wahrnehmungspsychologisch gleichmäßigen Helligkeitsinformation $L^*$ der \LAB- und \LUV-Farbräume (CIE-LAB und CIE-LUV), definiert als
%
\begin{align}
\mat I_{Lightness} &= \frac{1}{100}\left(100 \, f(\mat Y) - 16\right)
\end{align}
%
mit
%
\begin{align*}
\mat Y &= 0.2126 \mat R + 0.7152 \mat G + 0.0722 \mat B \\
f(t) &= \begin{cases}
		t^{\sfrac{1}{3}} & \text{wenn} \quad t > (\sfrac{6}{29})^3 \\
		\frac{1}{3}\left(\frac{29}{6} \right)^2 t + \frac{4}{29} & \text{sonst}
	\end{cases}
\end{align*}
%
wobei der Wertebereich auf $0 \dots 1$ normiert wurde (vgl. \cref{fig:conv_lightness}).
Diese Farbräume dekorrelieren Helligkeits- von Farbinformationen, wobei die a\textsuperscript{\tiny *}- und b\textsuperscript{\tiny *}-, bzw. u\textsuperscript{\tiny *}- und v\textsuperscript{\tiny *}-Achsen komplementäre Farbinformationen abbilden (etwa rot-grün und glau-gelb). \todo{Quelle}

\subsection{Wahl der Graustufenumwandlungsmethode}
\label{subsec:graustufen_wahl}

\Citeauthor{kanan2012color} ermitteln, dass die Wahl der der Umwandlungsmethode einen signifikanten Einfluss auf die erreichbaren Genauigkeiten verschiedener Algorithmen zur Merkmalsextraktion in Abhängigkeit von der Art der zu analysierenden Bilder hat.
Sie zeigen, dass sich gamma-korrigierende Umwandlungsverfahren wie \algo{Gleam} (oder ein nachträglich korrigiertes \algo{Intensity}) gegenüber allen nicht-gammakorrigierenden Verfahren wie \algo{Intensity}, \algo{Luster}, \algo{Luminance} etc. durchsetzen.
Hierbei ist die Methode \algo{Gleam} der beste Kandidat für Gesichts- und Objekterkennung, wobei eine gammakorrigierte Form von \algo{Luminance} bei Texturerkennungen hervorsticht.
Sie empfehlen ferner, jede Form von \algo{Value} (auch in Algorithmen zur Farbmerkmalsextraktion) durch \algo{Gleam} zu ersetzen.
Methoden, die sich geräteunabhängiger Farbräume und wahrnehmungspsychologischer Einflüsse bedienen, wie es bei \algo{Lightness} (\LAB, \LUV) der Fall ist, zeigen indes keinen besonderen Vorteil auf.

Die Entwicklung eines angepassten Umwandlungsalgorithmus
%
\begin{align}
\mat I_{opt} &=  \frac{1}{3}(\mat R^\frac{1}{a} + \mat G^\frac{1}{b} + \mat B^\frac{1}{b}) \label{eq:convert_opt}
\end{align}
%
mittels Optimierung der Parameter $a, b, c > 0$ speziell für gradienten- und kantenbasierte Detektoren (wie \algo{SIFT} und \algo{SURF}) wird vorgeschlagen.


\todo[inline]{Hinweis auf Vorteile von SIFT ggü. SURF}

\begin{figure}
	\centering
	\begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{conversion/rgb}
        \caption{\RGB}
        \label{fig:conv_rgb}
    \end{subfigure}
    \quad 
    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{conversion/intensity}
        \caption{Intensity}
        \label{fig:conv_intensity}
    \end{subfigure}

    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{conversion/value}
        \caption{Value (\HSV)}
        \label{fig:conv_value}
    \end{subfigure}
    \quad
    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{conversion/luster}
        \caption{"`Luster"' (\HLS)}
        \label{fig:conv_luster}
    \end{subfigure}

    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{conversion/luminance}
        \caption{Luminance}
        \label{fig:conv_luminance}
    \end{subfigure}
	\quad
    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{conversion/gleam}
        \caption{Gleam}
        \label{fig:conv_gleam}
    \end{subfigure}

    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{conversion/luma}
        \caption{Luma}
        \label{fig:conv_luma}
    \end{subfigure}
    \quad
    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{conversion/lightness}
        \caption{Lightness (\LAB, \LUV)}
        \label{fig:conv_lightness}
    \end{subfigure}

    \caption{Vergleich verschiedener Konvertierungen von Farb- zu Grauwerten}
    \caption*{Vergleich der in \cref{subsec:graustufen} beschriebenen Konvertierungen der \RGB-Farbkanäl von \cref{fig:conv_rgb} in Intensitätswerte.}
\end{figure}

\begin{lstlisting}[style=matlab,caption={Generieren einer Gamma-Lookuptabelle in Matlab}, label=lst:matlab_gamma]
gamma            = 1/2.2;
input_values     = 0:1:255;
correction_table = cast( 255*(input_values/255).^gamma, 'uint8')
\end{lstlisting}

\begin{figure}
	\centering
	
\begin{tikzpicture}[
	scale=1, 
	y=5cm,
	declare function={gamma(\t,\y)=\t^\y;}
	]
    
	\begin{axis}[
		grid=both,
    	axis lines=left,
	    enlargelimits=upper,
	    samples=100,
	    domain=0:1,
	    xmax = 1,
	    ymax = 1,
	    xlabel={$x$},
		ylabel={$\Gamma(x) = x^\gamma$},
	    legend pos=north west,
	    legend entries={$\gamma=\sfrac{1}{2.2}$, $\gamma=1.0$, $\gamma=2.2$}
		]

	\addplot [dotted, thick,smooth,red] {gamma(x,1/2.2)};
	\addplot [smooth, samples=10] {gamma(x,1.0)};	
	\addplot [dashed,smooth,blue] {gamma(x,2.2)};	
		
	\end{axis}
    
\end{tikzpicture}
	\caption{Gamma-Korrekturfunktionen verschiedener $\gamma$}
	\label{fig:gammaCorrections}
\end{figure}

\section{Normalisierung der Intensitäten}

\todo[inline]{Befreiung der Bildwerte von Mittelwert und Normalisierung durch Standardabweichung --> ist Unsinn}
\todo[inline]{Begründung und Quelle}

Der mittlere Intensitätswert $\mean{I}$ eines Bildes $\mat I$ der Dimension $M \times N$ berechnet sich als
%
\begin{align}
\mean{I} &= \frac{1}{M \times N} \sum_y \sum_x \mat I[x, y] \label{eq:mean}
\end{align}

Bei einer naiven Implementierung wird mit einer eine Doppelschleife über das Bild iteriert, die jeweilige Pixelintensität pro Kanal aufsummiert und anschließend mit der Anzahl der Bildwerte normiert.
Hierbei muss bedacht werden, dass ein Überlauf der Summenvariablen stattfinden kann; dass dies  bei geeigneter Wahl der Datentypen unwahrscheinlich ist, zeigt folgende Betrachtung:

Bei einem Bild mit 8 Bit Farbtiefe je Pixel (d.h. Wertebereich $0 \dots 255$ pro Kanal) ist die maximal erreichbare Summe $M \times N \times 255$.
Bei einem Bild mit \SI{4}{\mega\px} (z.B. $\num{2000} \times \SI{2000}{\px}$) entspricht dies dem Wert $\num{4000000} \times 255 = \num{1020000000}$.

Ein vorzeichenfreier 32-Bit-Datentyp (\datatype{unsigned int}) bildet jedoch bereits den Wertebereich $2^0 \dots 2^{32} = 0 \dots \num{4294967295}$ ab; das heißt, die maximale kumulative Intensität ist in diesem Fall ca. 3,3 Milliarden höher. 
Unter der Annahme einer vollen Aussteuerung (d.h. jedes Pixel desselben Kanals hat die Intensität $255$) ist der Wertebereich für Bilder bis $\sfrac{2^{32}}{255} \approx \SI{16843009}{\px}$ abgedeckt; dies entspricht ungefähr einem Bild der Dimension $\num{4104} \times \SI{4104}{\px}$.
Bei Verwendung eines vorzeichenfreien 64-Bit-Datentyps (\datatype{unsigned long}) beträgt der maximale Wert bereits
% $2^{64} = \num{18446744073709551616}$, d.h. 
ca. 18 Trillionen und entspräche bei gleichen Bedingungen 72 Billiarden Pixeln -- etwa einem Bild der Dimension $\num{268961285} \times \SI{268961285}{\px}$ und einem Speicherbedarf von ca. $\SI{72340172}{\gibi\byte}$ pro Kanal, wodurch sich die Überlaufproblematik erübrigt.
%Da heutige Prozessorarchitekturen \todo{Beispiele} auf die Verarbeitung von 64-Bit-Typen spezialisiert sind, ist ein solcher eine geeignete Wahl für eine derartige naive Implementierung.

\todo[inline]{Übergang zur Berechnung der (Stichproben-)Varianz (benötigt Erwartungs- oder Mittelwert) und Begründung für zweiphasige Berechnung, da Sobel-Operator.}
\todo[inline]{Beschreibung der on-line-Berechnung von Mittelwert und Standardabweichung}

Die Varianz der Bildintensitäten ist die mittlere quadratische Abweichung der einzelnen Intensitäten von ihrem Mittelwert $\mean{I}$:
%
\begin{align}
\sigma^2_I &= \frac{1}{M \times N} \sum_y \sum_x \left( \mat I[x, y] - \mean{I} \right)^2 \label{eq:variance}
\end{align}

Diese Formel setzt bei naiver Implementierung die Kenntnis des Mittelwertes $\mean{I}$ voraus und bedarf daher einer zusätzlichen Iteration über alle Pixel, wenn dies nicht der Fall ist.
Eine Form der on-line-Berechnung von Varianz und Mittelwert, die in nur eines Durchlaufes über alle Pixel bedarf, beschreibt \cite[216]{knuth85Seminumerical} als rekursive Berechnung über alle Bildwerte $x_k, k \in \left[1, M \times N \right]$ (\cref{eq:onlineVarianceM,eq:onlineVarianceS}), wobei sich die Varianz $\sigma^2_I$ gemäß \cref{eq:onlineVariance} ergibt:
%
\begin{align}
\begin{split}
	M_1 &= x_1 \\
	M_{k} &= M_{k-1} + \frac{(x_k - M_{k-1})}{k}
\end{split} \label{eq:onlineVarianceM} \\ \notag \\
\begin{split}
	S_1 &= 0 \\
	S_{k} &= S_{k-1} \left( x_k - M_{k-1} \right) \left( x_k - M_k \right)
\end{split} \label{eq:onlineVarianceS} \\ \notag \\
\begin{split}
	\mean{x}_n &= M_n \\
	\sigma^2_I &= \sigma^2_n = \frac{S_n}{n - 1} \quad \text{mit} \quad n = M \times N \label{eq:onlineVariance}
\end{split}
\end{align}
%
Nachteilig ist hierbei jedoch die in jedem Durchgang durchzuführende Division, welche die Verwendung von Fließkomma- oder Fixed-Point-Operationen erfordert, die ihrerseits sich aggregierende Rundungsfehler mit sich führen können.

\section{Cropping und Resampling}

\todo[inline]{Da die Ausgangsbilder im Wesentlichen aus (zum Großteil freigestellten) Produkt-Werbefotos bestehen ...}

\begin{figure}
	\centering
\begin{tikzpicture}
    
	\node[inner sep=0pt] (product) at (0,0)
    	{\includegraphics[width=.5\textwidth]{7A5F52750ADD07483305B0C2226A484ED74B42FD4D87B4BBBC352DCF4E8D8BB8}};
    	
	\draw[<->,shorten <= 1pt] (product.north west) -- (product.north east)
	    node[midway,fill=white] {\SI{1200}{\px}};    	
	\draw[<->,shorten <= 1pt] (product.north west) -- (product.south west)
	    node[rotate=90,midway,fill=white] {\SI{1450}{\px}};

	\coordinate (GL) at (product.south west);
	\coordinate (GR) at (product.south east);

	\coordinate (GT) at (product.north east);
	\coordinate (GB) at (product.south east);

	\coordinate (PL) at ($(GL)!115/1200!(GR)$);
	\coordinate (PR) at ($(GL)!1114/1200!(GR)$);

	\coordinate (PT) at ($(GT)!496/1450!(GB)$);
	\coordinate (PB) at ($(GT)!999/1450!(GB)$);

	\draw[<->] (PL) -- (PR)
	    node[midway,fill=white] {\SI{1009}{\px}};

	\draw[<->] (PT) -- (PB)
	    node[rotate=90,midway,fill=white] {\SI{514}{\px}};
	    
	\draw[dotted] (GL) -- (PL);
	\draw[dotted] (PR) -- (GR);
	\draw[dotted] (PB) -- (GR);
	\draw[dotted] (GT) -- (PT);
    	
\end{tikzpicture}
	\caption{Vorverarbeitung: Beispielbild Produktfoto}
	\caption*{Produkt "`Sonnenbrille, Farbverlauf, Leo, Havanna"', Michael Kors, \cite{Conleys:2015:Sonnenbrille:Online}}
	\label{fig:vorverBild}
\end{figure}

Das in \cref{fig:vorverBild} gezeigte Quellbild weist eine Dimension von $\SI{1200}{\px} \times \SI{1450}{\px}$ (Breite $\times$ Höhe) auf, von denen das eigentliche Produktbild jedoch nur ca. $\SI{1009}{\px} \times \SI{514}{\px}$ in Anspruch nimmt.

Würde dieses Bild nun zwecks Verringerung des Speicherbedarfs und der weiteren Verarbeitungszeiten auf eine maximale Kantenlänge von z.B. \SI{512}{\px} heruntergerechnet werden --- dies entspräche $\SI{424}{\px} \times \SI{512}{\px}$ --- so würde das eigentliche Produktbild im resultierenden Format nur noch $\SI{357}{\px} \times \SI{181}{\px}$ beanspruchen.

Durch ein zuvoriges Beschneiden des Bildes auf den relevanten Bildbereich 
-- eine genaue Definition des Begriffes "`relevant"' steht hierbei noch aus -- 
ergäbe sich indes ein Bereich von $\SI{512}{\px} \times \SI{261}{\px}$ für bildwichtige Inhalte.
Diese Vorgehensweise führt in diesem Beispiel folglich zu einer Verdoppelung der nutzbaren Datenmenge gegenüber den naiven Ansatz.
Wird statt der längsten Kante die kürzeste Kante normiert, ist die nutzbare resultierende Bildfläche um so höher, jedoch auch der zur Verarbeitung notwendige Rechenaufwand.
\Cref{tab:vorverDimensionen} verdeutlicht die Dimensionen und prozentualen Unterschiede.

Eines der Probleme bei der beschriebenen Vorgehensweise ist der Umstand, dass die kürzeste Kante kleiner als die gewünschte Normlänge, die längste Kante jedoch signifikant größer als diese sein kann.
Um solchen Fällen zu begegnen, werden daher die Kanten nicht direkt, sondern indirekt über die Bildfläche normiert.
Mit den Definitionen des Aspektverhältnisses $\alpha$ und der Bildfläche $F$ als
%
\begin{align}
\alpha &= \frac{N}{M} \\
F &= N \cdot M \label{eq:fläche}
\end{align}
%
sowie einer Zielfäche $F' = N' \cdot M'$ zu den normierten Kantenlängen $N', M'$ lassen sich mittels
%
\begin{align}
\frac{N'}{M'} &\overset{!}{=} \alpha \label{eq:nm_alpha} \\
N' \cdot M' &= N \cdot M \cdot \left( \frac{F'}{F} \right) \label{eq:nm_area}
\end{align}
%
über \cref{eq:fläche,eq:nm_alpha} in \cref{eq:nm_area} mit
%
\begin{align}
N' \cdot \left( \frac{N'}{\alpha} \right) = \frac{1}{\alpha} (N')^2 &= \cancel{F} \cdot \left( \frac{F'}{\cancel{F}} \right)
\end{align}
%
die normierten Kantenlängen $N'$ und $M'$ ermitteln als
%
\begin{align}
N' &= \sqrt{F' \cdot \alpha} \\
M' &= \frac{N'}{\alpha}
\end{align}

Durch dieses Verfahren lässt sich eine Obergrenze des Speicherbedarfes für die Bilddaten festlegen, wobei -- ausnehmlich Bildern mit $F < F'$ -- eine definierte Menge an Nutzdaten für die Weiterverarbeitung vorliegt.
Beispiel $P_5$ in \cref{tab:vorverDimensionen} demonstriert dieses Vorgehen, \cref{lst:matlab_simult_scale} zeigt eine Implementierung in MATLAB.

\begin{lstlisting}[style=matlab,caption={Simultane Skalierung der Bilddimensionen auf eine fixe Zielfläche}, label=lst:matlab_simult_scale]
w          = 1009;  % Bildbreite N
h          = 514;   % Bildhöhe M
targetArea = 512^2; % gewünschte Zielfläche

aspectRatio = w/h;
w2 = sqrt(targetArea*aspectRatio)
h2 = w2/aspectRatio
\end{lstlisting}

\begin{table}
	\centering
	\begin{tabular}{lrrrr}
		\toprule
		Bild & Breite & Höhe & Anzahl Pixel & Anteil Originalgröße \\
		\midrule
		$G_0$ & \SI{1200}{\px} & \SI{1450}{\px} & \SI{1740000}{\px} & \SI{100}{\percent} \\
		$P_0$ & \SI{1009}{\px} & \SI{514}{\px} & \SI{518626}{\px} & \SI{29.81}{\percent} \\
		\midrule
		$G_1$ & \SI{424}{\px} & \textbf{\SI{512}{\px}} & \SI{217088}{\px} & \SI{12.47}{\percent} \\	
		$P_1$ & \SI{357}{\px} & \SI{181}{\px} & \SI{64617}{\px} & \SI{3.71}{\percent} \\
		\midrule
		$G_2$ & \textbf{\SI{512}{\px}} & \SI{619}{\px} & \SI{316928}{\px} & \SI{18.21}{\percent} \\	
		$P_2$ & \SI{431}{\px} & \SI{219}{\px} & \SI{94389}{\px} & \SI{5.42}{\percent} \\		
		\midrule
		$G_3 = P_3$ & \textbf{\SI{512}{\px}} & \SI{261}{\px} & \SI{133632}{\px} & \SI{7.68}{\percent} \\
		$G_4 = P_4$ & \SI{1005}{\px} & \textbf{\SI{512}{\px}} & \SI{514560}{\px} & \SI{29.57}{\percent} \\
		\midrule
		$P_5$ & \textbf{\SI{718}{\px}} & \textbf{\SI{365}{\px}} & \SI{262070}{\px} & \SI{15.06}{\percent} \\
		\bottomrule
	\end{tabular}
	\caption{Vorverarbeitung: Bilddimensionen im Vergleich}
	\caption*{$G_0, P_0$: Bereich des Gesamtbildes $G$ und Produktbildes $P$ in den ursprünglichen Dimensionen; $G_1, P_1$: Bereiche nach Verkleinerung der längsten Kante des Bildes $G$ auf \SI{512}{px}; $G_2, P_2$: Bereiche nach Verkleinerung der kürzesten Kante des Bildes $G$ auf \SI{512}{px}; $G_3, P_3$: Bereiche nach Entfernung der bildunwichtigen Randteile in $G$, gefolgt von Reduktion der längsten Kante auf \SI{512}{\px}; $G_4, P_4$: Analog $G_3, P_3$, jedoch Reduktion der kürzesten Kante auf \SI{512}{\px}; $P_5$: Bestmögliche simultane Anpassung von Breite und Höhe auf eine Fläche von $F \approx \num{512}^2 \si{\px} = \SI{262144}{\px}$.}
	\label{tab:vorverDimensionen}
\end{table}

\section{Energiebasiertes Retargeting}

\todo[inline]{Auswertung des Beschnittbereiches über die erste Ableitung erlaubt es, einen adaptiven Threshold über die Bilddaten zu fahren; Farbverläufe -- ohnehin kein gutes Feature -- werden nicht als relevantes Merkmal erkannt und beschnitten. Dies beugt Belichtungsfehlern vor.}

Bildretargeting befasst sich mit der Aufgabe, ein Bild fester Dimension auf Zielen mit vom Bild unterschiedlicher oder generell variabler Dimension darzustellen, wie es etwa im Rahmen von responsive designs von Webseiten \todo{erklären} oder bei Smartphones beim Wechsel von vertikaler Lage ("`portrait mode"') zu horizontaler Lage ("`landscape mode"') notwendig wird.
%
Ein prominentes, aber nicht mehr all zu oft auftretendes Beispiel für dieses Problem ist die Darstellung von 4:3-Filmformaten auf einem Fernseher mit einem 16:9- oder 16:10-Bildverhältnis, wobei man sich zwischen vollem Bild, aber reduzierter Bildhöhe ("`letterbox"') oder voller Ausnutzung der Höhe der Anzeigefläche, aber fehlenden Bildrändern ("`Pan \& Scan"') entscheiden muss.

Während man einen Text dynamisch entsprechend des zur Darstellung verfügbaren Platzes umbrechen kann, ist dies bei Bildern nicht möglich.
Übliche Optionen in diesen Fällen sind das Beschneiden des Bildes auf die Dimensionen der Anzeigefläche (crop), das Skalieren des Bildes in den neuen Anzeigebereich unter Beachtung des Aspektverhältnisses (zoom, scale to fit), sowie das Skalieren des Bildes ohne Beachtung des Aspektverhältnisses (stretching), wobei jede dieser Optionen ihre eigenen Vor- und Nachteile mit sich bringt.

Ein wünschenswerter Zustand ist es, Bildteile stattdessen entsprechend ihrer Wichtigkeit zu behalten oder zu verwerfen. 
\cite{avidan2007seam} formulieren die "`Wichtigkeit"' eines Pixels als dessen Informationsgehalt, ausgedrückt über die Energie $\fun e_1(\mat I)$ des Bildes $\mat I$:
%
\begin{align}
\fun e_1(\mat I) &= |\frac{\partial}{\partial x} \mat I| + |\frac{\partial}{\partial y} \mat I| \label{eq:seamEnergy1}
\end{align}
%
Hierbei entsprechen die beiden Teilsummen den partiellen Ableitungen des Bildes in $x$- und $y$-Richtung.

\subsection{Umsetzung in der Vorverarbeitung}

Um die Vorgabe eines festen Schwellenwertes zu vermeiden,
werden die Randbereiche $\mat R_I$ des Bildes $\mat I$ ausgewertet,
wobei ein empirisch festgelegter Bereich von $\sfrac{1}{5}$ der Bildbreite und Höhe auf jeder Seite untersucht wird (in \cref{lst:matlab_energy_generate_energy} für direkte Schwellenwerte auf Intensitäten: $\sfrac{1}{6}$).
Der Median und die Standardabweichung der Werte wird ermittelt und der Offset $\mu$, sowie die Toleranz $\tau$ des Beschnitts ebenfalls empirisch wie folgt ermittelt:
%
\begin{align}
\mu &= 0 \\
\tau &= \median \mat R_I + 4 \sigma_R
\end{align}
%
wobei die Festlegung $\mu {=} 0$ gemäß \cref{eq:seamEnergy1} auf der Grundlage beruht, dass ein "`energiefreies"' Pixel den Wert $0$ besitzt.
In \cref{lst:matlab_energy_generate_energy} wurde für die Schwellenwerte der Intensitäten $\mu = \median \mat R_I$, sowie $\tau = \sqrt{\sigma_R}$ definiert, da sich hierbei in experimentellen Simulationen ein minimal besseres Ergebnis im Sinne des mittleren quadratischen Fehlers einstellte.
Die \Cref{fig:energycrop_real,fig:energycrop_simple} verdeutlichen das Verfahren,
wobei \cref{fig:energycrop_cumsum,fig:energycrop_toeplitz} anhand von synthetischen Testfällen die Schwachstellen des Beschnitts auf Intensitäten gegenüber dem Beschnitt auf den (differenzierenden) Energiewerten aufzeigen.
Die Ermittelung der Beschnittbereiche geschieht grundsätzlich gemäß wie folgt:
%
\begin{enumerate}
  \item Von links suchend wird die oberste Bildzeile $y_{top}$ ermittelt, deren Pixelwerte den Bereich $\mu \pm \tau$ verlassen.
  \item Von links suchend wird die unterste Bildzeile $y_{bottom}$ (bis ausschließlich $y_{top}$) ermittelt, deren Pixelwerte den Bereich $\mu \pm \tau$ verlassen.
  \item Von oben suchend wird die linkeste Bildspalte $x_{left}$ zwischen $y_{top}$ und $y_{bottom}$ ermittelt, deren Pixelwerte den Bereich $\mu \pm \tau$ verlassen.
  \item Von oben suchend wird die rechteste Bildspalte $x_{right}$ (bis ausschließlich $x_{left}$) wird zwischen $y_{top}$ und $y_{bottom}$ ermittelt, deren Pixelwerte den Bereich $\mu \pm \tau$ verlassen.
\end{enumerate}
%
Durch diese Vorgehensweise ist der Aufwand nur für den ersten Schritt maximal,
für die folgenden Schritte wird der zu durchsuchende Bereich sukzessive eingeschränkt.
Ein alternatives und effizienteres Vorgehen wäre das Durchsuchen des Bildes in initial diagonaler Richtung, gefolgt von einem "`backtracking"' gemäß des obigen Vorgehens in umgekehrter Richtung; 
dies setzt jedoch die (nicht zulässige) Grundannahme voraus,
dass das Bild einen soliden, zusammenhängenden Anteil besitzt, der durch die Bildmitte läuft.
Da die Art der Eingangsbilder nicht a priori bekannt ist, wird auf dieses Vorgehen verzichtet.

Das beschriebene spaltenweise Scannen des Bildes für $x_{left}$ und $x_{right}$ ist ein prinzipiell ineffizientes Verfahren, 
da mit jeder Selektion einer neuen Bildspalte ein cache miss riskiert wird, bei dem die Zeile zeitaufwändig aus dem Arbeitsspeicher nachgeladen werden muss.
Eine Möglichkeit besteht in der Transponierung des Bildblockes zwischen  $y_{top}$ und $y_{bottom}$, so dass die Spalten nun zeilenweise durchsucht werden können.
Umgesetzt wurde stattdessen folgender Ansatz:
%
\begin{enumerate}
  \item Von links suchend wird die linkeste Bildspalte $x_{left}$ zwischen $y_{top}$ und $y_{bottom}$ ermittelt, deren Pixelwerte den Bereich $\mu \pm \tau$ verlassen, wobei der Suchbereich sukzessive auf $0 \dots x_{left}$ eingeschränkt wird.
  \item Von rechts suchend wird die rechteste Bildspalte $x_{right}$ (bis ausschließlich $x_{left}$) zwischen $y_{top}$ und $y_{bottom}$ ermittelt, wobei der Suchbereich sukzessive auf $x_{right} \dots N$ eingeschränkt wird.
\end{enumerate}
%
\Cref{lst:matlab_energy_box} verdeutlicht das Vorgehen.

\subsection{Eindimensionale Faltung zur approximativen Differenzierung}

Eine Approximation der Ableitung $\sfrac{\partial \fun f}{\partial x}$ ist der zentrale Differenzenquotient
%
\begin{align}
\frac{\partial \fun f}{\partial x} &\approx \Delta f(x) = \frac{\fun f(x+\Delta x) - \fun f(x-\Delta x)}{2\Delta x} \label{eq:centralDifferenceOperator}
\end{align}
%
der sich bei einer diskreten Schrittweite von $\Delta x = \Delta k = 1$ als Faltung (\engl{convolution}) $(\fun f \ast \fun g)[k]$ von $\fun f[k]$ mit der Funktion
\begin{align}
\fun g[k] = 
	\begin{cases}
		-\frac{1}{2} & \text{wenn } k = -1 \\
		+\frac{1}{2} & \text{wenn } k = +1 \\
		0 & \text{sonst}
	\end{cases} \label{eq:conv1DKernel}
\end{align}
%
Die diskrete Faltung ist dabei definiert als:
%
\begin{align}
(\fun f \ast \fun g)[n] &= \sum_k \fun f[k] \cdot \fun g[n - k] \\
              &= \sum_k \fun f[n - k] \cdot \fun g[k] \label{eq:diskFaltung}
\end{align}

Es zeigt sich, dass diese einer gewichteten Summenbildung der Funktion $f$ mit allen Gewichtswerten $g$ entspricht%
\footnote{Der Umkehrung der Betrachtungsrichtung von $k$ zu $-k$ verdient die Faltung ihren Namen, analog einem gefalzten Blatt, dessen eine Hälfte auf die andere gefaltet wird.}.
Die Variation in \cref{eq:diskFaltung} ergibt sich aus der Kommutativität der Faltungsoperation und ist oft besser geeignet, um Gewichtungen mit einer begrenzten Anzahl an Koeffizienten (z.B. einer finiten Impulsantwort) darzustellen%
\footnote{Die Mächtigkeit der Menge der Gewichtungskoeffizienten $g$ ist im Allgemeinen geringer als die Mächtigkeit der Menge der zu gewichtenden Funktionswerte $f$; ein FIR-Filter ist üblicherweise "`kürzer"' als das zu filternde Signal.}.

\Cref{eq:comparisonConv1DWithCentralDiff} zeigt die Identität zwischen der Faltung einer Funktion $f[n]$ mit der in \cref{eq:conv1DKernel} dargestellten Funktion $g[n]$:
%
\begin{align}
\label{eq:comparisonConv1DWithCentralDiff}
\begin{split}
(\fun f \ast \fun g)[n]
                &= \overbrace{\fun f\left[n - \left(-1\right)\right] g[-1]}^{k = -1}
              \,+\, \overbrace{\fun f\left[n - 0\right] g[0]}^{k = 0}
              \,+\, \overbrace{\fun f\left[n - \left(+1\right)\right] g[+1]}^{k = +1} \\
              &= \fun f\left[n + 1\right] \cdot \left(+\frac{1}{2}\right) 
              + \fun f\left[n\right] \cdot (0) 
              + \fun f\left[n - 1\right] \cdot \left(-\frac{1}{2}\right) \\              
              &= \frac{\fun f\left[n + 1\right] - \fun f\left[n - 1\right]}{2}
\end{split}
\end{align}

Bei Darstellung der Funktion $f[n]$ und des Faltungskerns $g[n]$ in vektorieller Form als $\vec f$ und $\mat g$ schreibt sich die Faltung generalisiert auf eine oder zwei Dimensionen als
%
\begin{align}
(\fun f \ast \fun g)(n) = \mat f \ast \vec g
\end{align}
%
Der in \cref{eq:conv1DKernel} beschriebene eindimensionale Faltungskern zur Berechnung des zentralen Differenzenquotienten zur diskreten Approximation des Differentialquotienten gemäß \cref{eq:centralDifferenceOperator} beschreibt sich so als
%
\begin{align}
\mat g_{diff} = \frac{1}{2} \begin{bmatrix}
 -1 & 0 & +1
 \end{bmatrix} \label{eq:len3diffKernel}
\end{align}

Da Differenzierungsverfahren auf abrupte Werteveränderungen entsprechend stark reagieren, sind sie anfällig gegenüber Störprozessen, die das Nutzsignal überlagern.
Dieses generelle Problem der stochastischen%
\footnote{Ist die Überlagerung eines Störprozesses exakt beschreib- und damit korrigierbar, handelt es sich nicht länger um ein stochastisches, sondern deterministisches Signal; in Folge der a priori-Kenntnis des Inhaltes erübrigt sich dann dessen Verarbeitung.}
Signalverarbeitung kann durch ein Glätten des Signales gemindert werden:
Hierbei wird die relative Veränderung eines Wertes zu seinen Nachbarwerten beschränkt, wodurch die Amplitude des Rauschsignales auf Kosten der Amplitude des Nutzsignales gemindert wird.
Eine zugrunde liegende Annahme dürfte sein, dass natürlich vorkommende -- genereller: energetisch günstige -- Folgezustände nicht stark variieren%
\footnote{"`Natura non facit saltus"', Die Natur macht keine Sprünge; Carl von Linné.}, ein Wert also im Allgemeinen ähnlich zu seiner Umgebung ist.

Einen einfachen, aber effektiven Glättungsprozess stellt dabei der (lokal) gewichtete Mittelwert dar, welcher die Intensität eines Wertes -- z.B. Pixels -- aus seiner Nachbarschaft ermittelt.
Zwei eindimensionale Glättungskern in horizontaler Richtung stellen \cref{eq:len3meanKernel1D} (ein einfacher Mittelwert über die direkte Nachbarschaft), sowie \cref{eq:len3smoothKernel1D} (ein gewichteter Mittelwert über dieselbe Nachbarschaft) dar:
%
\begin{align}
\mat g_{mean} = \frac{1}{3} \begin{bmatrix}
 1 & 1 & 1
 \end{bmatrix} \label{eq:len3meanKernel1D} \\
\mat g_{smooth} = \frac{1}{4} \begin{bmatrix}
 1 & 2 & 1
 \end{bmatrix} \label{eq:len3smoothKernel1D}
\end{align}

In jedem Fall kollidiert jedoch die Aufgabe der Differenzierung in Laufrichtung des Filters mit der Glättung in derselben Richtung, da beide Operationen gegenteilige Ziele verfolgen (jener die Detektion, dieser die Minderung von Sprüngen).

\subsection{Sobel-Operator und zweidimensionale Faltung}

Der \algo{Sobel-Operator} (nach \cite{Sobel:1968:Talk}, beschrieben in \cite{Sobel:2015:Online}) löst dieses Problem im zweidimensionalen Raum, indem er die Glättungsoperation stattdessen orthogonal zur Suchrichtung ausführt.
%Soll in horizontaler Richtung differenziert werden, wird hierzu ("`zuerst"') in vertikaler Richtung geglättet.
%
Dabei bedient er sich der zweidimensionalen diskreten Faltung
%
\begin{align}
(\fun f \ast \fun g)[m, n] &= \sum_k \sum_l \fun f[k, l] \cdot \fun g[m - k, n - l] \label{eq:diskFaltung2D}
\end{align}
%
welche einen zweidimensionalen Filterkern (eine Matrix) auf eine zweidimensionale Funktion (ebenfalls eine Matrix) anwendet.

Der 2D-Filterkern des Sobel-Operators zur Differenzierung in horizontaler Richtung $\mat G_{x}$ ergibt sich nun aus der Überlagerung der Kerne $\mat g_{smooth}$ und $\mat g_{diff}$ mittels des dyadischen (bzw. Kronecker-)Produktes als
%
\begin{align}
\mat G_{x} &= \frac{1}{8}
\begin{bmatrix}
 -1 & 0 & 1 \\
 -2 & 0 & 2 \\
 -1 & 0 & 1
 \end{bmatrix} = \mat g_{smooth}^T \otimes \mat g_{diff} \label{eq:sobel3x}
\end{align}

Analog existiert ein Operator $\mat G_{y}$ für Differenzierung in vertikaler Richtung mit
%
\begin{align}
\mat G_{y} &= \frac{1}{8}
\begin{bmatrix}
 -1 & -2 & -1 \\
 0 & 0 & 0 \\
 1 & 2 & 1
 \end{bmatrix} = \mat g_{diff}^T \otimes \mat g_{smooth} \label{eq:sobel3y}
\end{align}

Der Vektor der partiellen Ableitungen $\frac{\partial \mat I}{\partial x}$, $\frac{\partial \mat I}{\partial y}$ beschreibt den Gradienten $\nabla \mat I$ eines Bildes
%
\begin{align}
\nabla \mat I = 
	\begin{bmatrix}
		\sfrac{\partial \mat I}{\partial x} \\
		\sfrac{\partial \mat I}{\partial y}
	\end{bmatrix} \approx
	\begin{bmatrix}
		\Delta_x \mat I \\
		\Delta_y \mat I
	\end{bmatrix}  \label{eq:gradient}
\end{align}
%
welcher anschaulich eine Menge von Richtungsvektoren beschreibt, entlang derer die einzelnen Pixel des Bildes ausgerichtet sind; der Sobel-Operator beschreibt eine mögliche Approximation des "`echten"' Gradienten.

Eine nennenswerte Eigenschaft der Differenzierung von Bildern ist, dass die so ermittelten Intensitätssprünge üblicherweise \todo{Quelle} einhergehen mit dem Vorhandensein von Kanten abgebildeter Objekte, weswegen der Sobel-Operator regelmäßig als einfacher%
\footnote{Der Sobel-Operator ist nicht anisotrop; seine Sensitivität gegenüber diagonalen Kanten -- insbesonderer solcher im 45°-Winkel zu den beiden Suchrichtungen -- ist minimal.}, wenngleich effizienter Kantendetektor eingesetzt wird.
\Cref{fig:sobelGlasses,fig:sobelMascara} demonstrieren die Wirkung des Sobel-Operators.

Analog \cref{eq:seamEnergy1} ergibt sich nun das Energiebild des Bildes $\mat I$ als
%
\begin{align}
\begin{split}
\fun e_1(\mat I) &\approx |\mat G_x \ast \mat I| + |\mat G_y \ast \mat I| \\
                 &= |\Delta_x \mat I| + |\Delta_y \mat I|
\end{split} \label{eq:sobelEnergy}
\end{align}
%
das heißt der absoluten Summe des mit jedem Sobel-Operatoren separat gefalteten Bildes.
\Cref{fig:mascaraEnergy} zeigt exemplarisch das Energiebild von \cref{fig:mascara}.

\todo[inline]{Weitere Energie ergibt sich über Grauwert-Cooccurence-Matrix, siehe \url{http://de.mathworks.com/help/images/ref/graycoprops.html} und 
\url{http://de.mathworks.com/help/images/ref/graycomatrix.html}}
\todo[inline]{Referenz auf Quadrat der Beträge aus Wahrscheinlichkeit des Intensitätswertes -- analog Cooccurence \url{http://stackoverflow.com/a/34273211}}

\subsection{Separierbare Faltungskerne}

Ein zweidimensionales Filter, das wie der Sobel-Operator aus dem dyadischen (bzw. Kronecker-) Produkt $\vec a \otimes \vec b$ zweier Vektoren $\vec a, \vec b$ zusammengesetzt ist, wird separierbar genannt. \todo{Quelle}
\todo[inline]{Jede Matrix des Ranges 1 ist separierbar und kann mittels Singulärwertzerlegung in ihre äußeren Faktoren zerlegt werden. -- Quelle! (Wikipedia ...)}

Da die Faltung nicht nur kommutativ, sondern auch assoziativ ist -- $(\fun f \ast \fun g) \ast \fun h = \fun f \ast (\fun g \ast \fun h)$ -- gilt für separierbare Kerne $\mat G$
%
\begin{align}
\mat G \ast \mat I = (\vec g^T \ast \vec h) \ast \mat I = (\vec g^T \ast \mat I) \ast \vec h
\end{align}
%
Die Faltung der Matrix $\mat I$ mit einem separierbaren Filter $\mat G = \vec g^T \ast \vec h$ ist folglich identisch mit der Faltung der Matrix $\mat I$ mit dem Vektor $\vec g^T$, gefolgt von der Faltung des Ergebnisses mit dem Vektor $\mat h$.

\label{conv2d_mnpq} 
Die Faltung einer $M \times N$-Matrix (z.B. eines Grauwertbildes) mit einem $P \times Q$-Filterkern benötigt bei einfacher Implementierung%
\footnote{Die Faltung entspricht der Multiplikation der fouriertransformierten, weswegen es effizient sein kann, die Faltung vollständig im Fourierraum durchzuführen.}
$M \times N \times P \times Q$ Multiplikationen und Additionen (jeder Wert der Matrix muss mit jedem Wert des Kernes multipliziert werden). 
Unter Ausnutzung der Separierbarkeit kann die Filterung in zwei Schritten zu je $M \times N \times P$ bzw. $M \times N \times Q$ Operationen durchgeführt werden; in Summe werden hierfür demnach $M N P + M N Q = M N (P + Q)$ Operationen benötigt.
Separierbare Filter können folglich um Faktor $\sfrac{P Q}{(P + Q)}$ schneller angewandt werden:
Für einen $3 \times 3$-Filterkern ergibt sich eine Beschleunigung um Faktor $1.5$, für $5 \times 5$-Filterkerne bereits ein Speedup von $2.5$.

\begin{figure}
	\centering
	\begin{subfigure}[b]{0.8\textwidth}
        \includegraphics[width=\textwidth]{7A5F52750ADD07483305B0C2226A484ED74B42FD4D87B4BBBC352DCF4E8D8BB8-sobelX}
        \caption{Ausschnitt gefiltert mit horizontalem Sobel-Operator, \cref{eq:sobel3x}}
        \label{fig:sobelX}
    \end{subfigure}
    
    \begin{subfigure}[b]{0.8\textwidth}
        \includegraphics[width=\textwidth]{7A5F52750ADD07483305B0C2226A484ED74B42FD4D87B4BBBC352DCF4E8D8BB8-sobelY}
        \caption{Ausschnitt gefiltert mit vertikalem Sobel-Operator, \cref{eq:sobel3y}}
        \label{fig:sobelY}
    \end{subfigure}

    \caption{\Cref{fig:vorverBild} gefiltert mit Sobel-Operator}
    \caption*{Nur der zentrale Bildausschnitt wird dargestellt. Eine Skalierung um $\sfrac{1}{8}$ und Offsetverschiebung um $+127$ zurück in den Wertebereich $0 \dots 255$ ($0$ entspricht schwarz, $255$ entspricht weiß) wurde für Darstellungszwecke vorgenommen; Neutralgrau (Helligkeitswert $127$) entspricht Kantenfreiheit, hellere Töne einer steigenden Flanke (Suchrichtung links nach rechts bzw. oben nach unten), dunklere Töne einer fallenden Flanke.}
    \label{fig:sobelGlasses}
\end{figure}

\begin{figure}
	\centering
	\begin{subfigure}[b]{0.25\textwidth}
        \includegraphics[width=\textwidth]{E559256E6CEA31B79B411104084DF35F9370B147FA3E9BF668FD2FE07367702D}
        \caption{Original} \label{fig:mascara}
    \end{subfigure}
    %
    \begin{subfigure}[b]{0.25\textwidth}
        \includegraphics[width=\textwidth]{E559256E6CEA31B79B411104084DF35F9370B147FA3E9BF668FD2FE07367702D-intensity}
        \caption{Intensity} \label{fig:mascara_intensity}
    \end{subfigure}
    %
    \begin{subfigure}[b]{0.25\textwidth}
        \includegraphics[width=\textwidth]{E559256E6CEA31B79B411104084DF35F9370B147FA3E9BF668FD2FE07367702D-gleam}
        \caption{Gleam} \label{fig:mascara_gleam}
    \end{subfigure}
           
    \caption{Graustufenumwandlung Intensity und Gleam}
    \caption*{Umwandlung des Originalbildes mittels Intensity- und Gleam-Methode. \\ \centering Mascara, \cite{Lacome:2015:Mascara:Online}}
\end{figure}

\begin{figure}
	\centering
    \begin{subfigure}[b]{0.25\textwidth}
        \includegraphics[width=\textwidth]{E559256E6CEA31B79B411104084DF35F9370B147FA3E9BF668FD2FE07367702D-sobelX}
        \caption{$\Delta_x \mat I$}
        \label{fig:mascaraSobelX}
    \end{subfigure}
    \quad
    \begin{subfigure}[b]{0.25\textwidth}
        \includegraphics[width=\textwidth]{E559256E6CEA31B79B411104084DF35F9370B147FA3E9BF668FD2FE07367702D-sobelY}
        \caption{$\Delta_y \mat I$}
        \label{fig:mascaraSobelY}
    \end{subfigure}    
    \quad
    \begin{subfigure}[b]{0.25\textwidth}
        \includegraphics[width=\textwidth]{E559256E6CEA31B79B411104084DF35F9370B147FA3E9BF668FD2FE07367702D-energy-norm}
        \caption{$|\Delta_x \mat I| + |\Delta_y \mat I|$}
        \label{fig:mascaraEnergy}
    \end{subfigure}     
           
    \caption{\Cref{fig:mascara} gefiltert mit Sobel-Operator, Energiebild}
    \caption*{Anwendung der Sobel-Operatoren aus \cref{eq:sobel3x,eq:sobel3y} in X- und Y-Richtung auf \cref{fig:mascara}, sowie Energiebild gemäß \cref{eq:sobelEnergy} (normalisiert für Darstellung).
    Normierung auf Wertebereich $0 \dots 255$ analog \cref{fig:sobelGlasses}.}
    \label{fig:sobelMascara}
\end{figure}

\todo[inline]{Stellt Kanten mit doppelter Breite dar}
\todo[inline]{Darstellung als Stems und Pixelwerte!}
\todo[inline]{Sinnvolle Begründung für zentralen Differenzenquotienten}
\todo[inline]{Intuitive Erklärung für zentralen Differenzenquotienten}
\todo[inline]{Sample und Plots für gefilterte Daten}

\chapter{Übersicht der Literatur}

\cite{orb} beschreiben einen neuen binären Deskriptor genannt \algo{ORB}, der auf \algo{BRIEF} basiert, jedoch um Rotationsinvarianz und Rauschresistenz erweitert wurde. \algo{ORB} ist (bei vergleichbarer Performance) um zwei Magnituden schneller als \acro{SIFT}. Unterstellt dabei, dass \algo{SURF} besser ist als \algo{SIFT}; liefert Quelle für \algo{SURF}.

\cite{eliveindonesia} ist eine Präsentation zum Entwurf eines Nutz"-vieh-Klas"-si"-fi"-ka"-tions"-sys"-tems, das Ansätze zur Parallelisierung von \algo{SIFT} und \algo{SURF} anmerkt. Hierbei werden unter anderem Parallelisierungen der Algorithmen (\algo{Parallel SIFT} und \algo{Parallel SURF}) besprochen, sowie auf Mehrmaschinen-Verarbeitung der Bilder selbst angeht; Hintergrund ist, dass die Features selbst lokaler Natur sind, für die Auswertung also nicht unbedingt der gesamte Kontext des Bildes notwendig ist. Eine Einführung von Padding an den Bildrändern wird ebenfalls erwähnt.

\cite{detmatchkeyproadPresi} vergleichen \algo{SIFT} mit \algo{Affine-SIFT} und anderen (Stichworte \algo{FLANN} und \algo{RANSAC} -- Hinweis: \algo{FLANN} ist auch als Functional Link Neural Network bekannt, meint hier jedoch Fast Linear Approximation of Nearest Neighbors) und weist darauf hin, dass \algo{SIFT} nur in vier Parametern invariant ist: Zoom, Rotation, sowie X- und Y-Translation; affine Transformationen wie Perspektivenänderungen oder Rotation um eine geneigte Kameraachse sind dabei inbegriffen (d.h. gegenüber Kameratilts ist Standard-\algo{SIFT} nicht invariant).

Hinweis auf den "`Curse of dimensionality"' (den "`Fluch der Dimensionalität"'), 
siehe \url{https://en.wikipedia.org/wiki/Curse_of_dimensionality}, zur Begründung,
weswegen Kd-Trees prinzipiell ungeeignet für das Indexieren von \algo{SIFT}-Features ist (diese sind sehr hochdimensional).

\cite{zha2010computer} beschreiben als \cite{zha2010computer_mift} (siehe \url{http://link.springer.com/chapter/10.1007/978-3-642-12304-7_50}) den \algo{MIFT}-Algorithmus "`A Mirror Reflection Invariant Feature Descriptor"'.

Zusätzliche Entwicklung mit \algo{MI-SIFT}.

\cite{alhwarin2011fast} nennt drei neue \algo{SIFT}-Ansätze für schnelleres und robusteres Matching in Robotik-Anwendungen.

\cite{yin2013content} beschreiben Content-Based Image Retrieval mittels Apache Hadoop auf Basis von \algo{SURF}-Deskriptoren und "`Locality-Sensitive Hashing"'.

\cite{ginesu2012objective} vergleichen \algo{WebP} mit bestehenden Bildformaten wie \algo{JPEG} und \algo{PNG}.

\cite[395]{corke2011robotics} beschreibt \algo{RANSAC} in MATLAB.
\cite[373]{corke2011robotics} beschreibt Featureextraktion.

\cite{87c32622126d4fc1a7cb3bcca3ecd7d4} beschreiben eine Methode zum Bewerten/Gewichten/Ranking der gefundenen Features durch eine mehrfache (Re-)Detektion bei synthetisch modifizierten Bildern - d.h. Modifikation der Belichtung (z.B. Gamma), der Rotation, der Skalierung, etc.; diejenigen Feature, die in allen oder möglichst vielen Variationen wiedergefunden werden, werden höher bewertet als der Rest.

\url{http://www.crisluengo.net/index.php/archives/22}: "`Meaning that by convolving an image repeatedly with a uniform filter we can approximate a Gaussian filter. It also means that convolving an image twice with a Gaussian filter yields an image filtered with a larger Gaussian filter. The size parameter $\sigma$ of this resulting larger Gaussian filter can be computed by quadratically adding the $\sigma$s of the applied filters: $\sigma_{total} = \sqrt{\sigma^2_1+\sigma^2_2}$. So when repeatedly convolving an image with the same Gaussian, we effectively increase the $\sigma$ with a factor $\sqrt{2}$ with every convolution."'

\url{http://www.nowozin.net/sebastian/tu-berlin-2006/autopano-sift/technicaldetails.html} verweist auf \algo{Best Bin First} nearest neighbor-Suche in \texttt{autopano-sift}.

\cite[319ff]{das2015guide} beschreibt hübsch, aber nicht sehr hilfreich \algo{SIFT}.

\cite{DBLP:conf/isvc/BelaroussiM08a} beschreiben als "`Image-Based Information Guide on Mobile Devices"' u.a. ein ggü. \algo{RANSAC} verbessertes Matching.

\cite[328ff]{DBLP:conf/isvc/HeW08} beschreiben als "`A Fast and Effective Dichotomy Based Hash Algorithm for Image Matching"' eine schnellere Alternative zum \algo{Best-Bin-First KNN} für \algo{SIFT}.

\url{https://homepages.cae.wisc.edu/~ece533/images/} Public-Domain Test Images for Homeworks and Projects: Lena und Barbara (Aliasing!)

\url{http://dsp.stackexchange.com/questions/28312} Eigene Frage zu $\sigma=0.5$ bezüglich \cite{lowe2004distinctive}.

\todo[inline]{Bett wiederfinden bei IKEA - "`markanter"' Knauf wird vermutlich nicht gefunden, da Query-Bild selbst viele irreführende Feature beinhaltet. Idee: Location Cues verwenden, bei dem über einen geo-lookup ermittelt wird, dass sich der Nutzer an einem IKEA-Standpunkt befindet; Bilder aus diesem Laden werden dann bevorzugt bewertet.}

\todo[inline]{EXIF-Daten des Handies hinterlegen (insbesondere Modell bzw. Kameramodell) um ggf. Korrekturen der Optik vornehmen zu können -- Grundproblem: Blurannahme von \algo{SIFT} entscheidet maßgeblich über Repeatability (wo ist das Paper?).}

\todo[inline]{\algo{SIFT}-Suche überspringt Bildränder. Beim Crop daher etwas Rand übrig lassen.}


\chapter{Scale-Invariant Feature Transform}

Scale-Invariant Feature Transform (\algo{SIFT}), zu Deutsch "`Skaleninvariante Merkmalstransformation"', 
ist ein in \cite{lowe1999recognition,lowe2004distinctive} vorgestellter Detektor und Deskriptor für markante lokale Mermale in Bildern, den sogenannten \textit{keypoints},
der dem menschlichen Sehempfinden nachempfunden wurde und invariant gegenüber Kontrast-, Translations-, Rotations- und Skalierungsunterschieden sein soll\todo{nicht jedoch vollständig affine Transformationen, siehe ASIFT}.

Im Gegensatz zu globalen Mermalen eines Bildes (Größe, Histo- oder Farbkorrellogramme, ...) deren Erscheinung unter Einfluss selbst einfacher Transformationen 
wie der Rotation des Bildes um \SI{90}{\degree}, einer Skalierung um \SI{50}{\percent}, der Reduktion des Kontrastes, etc. stark variieren kann, 
beschränkt \algo{SIFT} sich auf die Erkennung lokaler Merkmale\footnote{Klassische lokale Merkmale eines Bildes sind Ecken und Kanten der abgebildeten Objekte, die über den Bildgradienten ermittelt werden können.} eines Bildes. 
\Citeauthor{lowe1999recognition} merkt dabei an, 
dass Detektoren, die ausschließlich auf Ecken- und Kanteninformationen basieren -- wie etwa der Moravec- oder Harris-Operator -- nicht invariant gegenüber Skalierungen des Bildes sind 
und somit in keiner stabilen Merkmalserkennung bei etwa aus unterschiedlichen Distanzen aufgenommenen Bildern desselben Objektes resultieren.
Er führt daher die Merkmalserkennung nicht auf den Bilddaten selbst, sondern innerhalb des sog. Skalenraums aus.
\Cite{Witkin:1983:SF:1623516.1623607,1172729} definiert den Skalenraum (engl. \textit{Scale-Space}) eines Signals $f(z)$ als die Menge der Faltungen des Signals mit einem isotropen (richtungsinvarianten) Skalierungsoperator $g(z,\sigma)$ über alle Skalierung $\sigma$ als
%
\begin{align}
\phi(x,y,\sigma) &= \fun f(x,y,\sigma) \ast \fun g(x,y,\sigma) \label{eq:scale-space}
\end{align}
%
wobei jede Skale $\sigma$ im weitesten Sinne einem entsprechendem Betrachtungsabstand des abgebildeten Objektes entspricht.
Er wählt für diesen Operator die Gaußfunktion\footnote{Der Gaußoperator ist die Wahrscheinlichkeitsdichtefunktion der Normalverteilung.} für mittelwertfreie Signale mit einheitlicher Standardabweichung (d.h. Skale) in den räumlichen Dimensionen
%
\begin{align}
\fun g_{Gauss}(x,y,\sigma) &= \frac{1}{\sigma\sqrt{2\pi}} \myexp{-\frac{x^2 + y^2}{2 \sigma^2}} \label{eq:gaussian_2d}
\end{align}
%
und zeigt mit \cite{4767749}, dass dieser der \textit{einzige} für die Ermittlung des Skalenraums in Frage kommende Operator ist, so dass mit \cref{eq:scale-space,eq:gaussian_2d} gilt
%
\begin{align}
\mat \Phi[x,y,\sigma] &= \mat I[x,y] \ast \mat G_{Gauss}[x,y,\sigma]
\end{align}
%
\Citeauthor{Witkin:1983:SF:1623516.1623607} stellt fest, dass Nulldurchgänge der Faltung und ihrer Ableitungen im Skalenraum (d.h. Kanten) ab einer bestimmten Skale $\sigma$ auftreten und für alle niedrigeren Skalen erhalten bleiben (die Frequenzauflösung steigt mit kleiner werdendem $\sigma$). \Cref{fig:scalespace_contours} dies in einem Konturdiagramm von $\nabla^2 \mat \Phi[x,y,\sigma]$ dar.
Die in $\Phi[x,y,\sigma]$ gefundenen \algo{SIFT}-Merkmale sind somit prinzipbedingt keine zweidimensionalen Merkmale in $x,y$, sondern dreidimensional in $x,y,\sigma$.

\begin{figure}
	\centering
	\begin{subfigure}[b]{\textwidth}
		\centering
		\includegraphics[width=0.8\textwidth]{scale-space/signal-contours}
		\caption{Signal (unten) und dazugehörige Konturen der Nulldurchgänge der zweiten Ableitung (oben).}
		\caption*{Die Konturen beschreiben das Verhalten der Nulldurchgänge von $\sfrac{\partial^2}{\partial x^2} \Phi[x,y,\sigma]$ des Signals (unterer Graph) bei variierendem $\sigma$. Größere $\sigma$ (d.h. größere Skalen) finden sich in nach oben steigender Richtung, kleinere $\sigma$ darunter. Wendepunkte in $\Phi[x,y,\sigma]$ treten bei einem bestimmten $\sigma_A$ auf und bleiben für alle $\sigma<\sigma_A$ erhalten, d.h. ein in $\sigma_A$ gefundenes Merkmal wird auch in allen $\sigma<\sigma_A$ detektierbar sein.}
	\end{subfigure}	

	\begin{subfigure}[b]{\textwidth}
		\centering
		\includegraphics[width=0.8\textwidth]{scale-space/dumbell-contours}
		\caption{Hantelfigur und Verlauf der Kantenkonturen bei wachsender Skale.}
		\caption*{Zusammenführung der $\sfrac{\partial^2}{\partial x^2} \Phi[x,y,\sigma]$-Nulldurchgänge bei wachsender Skale. Schwarz und weiß repräsentieren die Werte $0$ und $1$, von links nach rechts wird für jedes Bild der Sequenz der Wert von $\sigma$ verdoppelt.}
	\end{subfigure}
		
	\caption{Verhalten der Nulldurchgänge im Skalenraum}
	\caption*{\cite{img:4767749}}
	\label{fig:scalespace_contours}
\end{figure}

\section{Laplacian of Gaussian als Difference of Gaussian}

Die Extremadetektion im Skalenraum basiert nach wie vor auf einer Kantendetektion, bei welcher der Laplace-Operator (d.h. die zweite Ableitung des Bildes) zur Detektion der Kantenpositionen verwendet wird.
Dieser ist aufgrund seiner Isotropie  einer gerichteter Ableitung, 
wie sie etwa durch den Sobel-Operator (vgl. \cref{eq:gradient}) erreicht wird, vorzuziehen. 
Wenngleich die zweifache Differenzierung prinzipbedingt überlagernde Störsignale\todo{Faktor} erneut gegenüber einem einfach differenzierendem Operator verstärkt,
wird dieser Sachverhalt durch die bereits erfolgte Glättung mittels des Gaußoperators gemildert.
%
Die Gewinnung der Parameter für den Laplace-Operator ist nicht eindeutig; 
zwei Varianten eines Laplace-Operatorkernes (\cite{sonka2008image}) sind:
%
\begin{align}
\mat G_{Laplace,1} = \begin{bmatrix}
	0 & 1 & 0 \\
	1 & -4 & 1 \\
	0 & 1 & 0 \\
	\end{bmatrix} , & \quad
\mat G_{Laplace,2} = \begin{bmatrix}
	1 & 1 & 1 \\
	1 & -8 & 1 \\
	1 & 1 & 1 \\
	\end{bmatrix}
\end{align}
%
Das Ergebnis einer Laplace-Filterung des mit einem Gaußoperator geglätteten Bildes (Laplacian-of-Gaussian, \LoG) ist als Marr-Hildreth-Operator oder Mexican Hat-Kern%
\footnote{Der Verlauf der Funktionswerte ähnelt der Form eines (auf dem Kopf stehenden) Sombreros, vgl. dazu \cref{fig:log}.} 
bekannt (\cite{corke2011robotics}). 
Folglich gilt
%
\begin{align}
\mat G_{LoG} = \mat G_{Gauss} \ast \mat G_{Laplace}
\end{align}
%
Der Laplace-Operator selbst ist nicht separierbar (der Rang der Matrix ist größer als~$1$), wodurch dies ebenfalls für den \LoG-Operator gilt, was seine Anwendung kostspielig macht (vgl. \cref{subsec:scale_space_sift}).
Analytisch ergibt sich der \LoG-Operator als
%
\begin{align}
\fun g_{LoG}(x,y,\sigma) &= \nabla^2 \fun g_{Gauss}(x,y,\sigma) \\
	&= \frac{\partial^2}{\partial x^2} \fun g_{Gauss}(x,y,\sigma) + \frac{\partial^2}{\partial y^2} \fun g_{Gauss}(x,y,\sigma)\\
	&= \frac{1}{\pi\sigma^4} \parens{\frac{x^2+y^2}{2\sigma^2}\myexp{-\frac{x^2+y^2}{2\sigma^2}}}
\end{align}

\Cite{lowe1999recognition} macht sich zunutze, dass die Faltung eines Bildes mit dem \LoG-Operator angenähert werden kann durch die Subtraktion zweier durch den Skalenraum ohnehin vorhandenen mit dem Gaußoperator geglätteten Bilder;
das Verfahren heißt entsprechend Difference-of-Gaussian (\DoG).
Es gilt:
%
\begin{align}
\mat L[\sigma] &= \mat I \ast \mat G_{LoG}[\sigma] \\
	&\approx 
	  \mat I \ast \mat G_{Gauss}[k\sigma]
	- \mat I \ast \mat G_{Gauss}[\sigma]
\end{align}
%
mit $k>1$, wobei für eine gute Approximation üblicherweise $k \approx 1.6$ angenommen wird (vgl. \cite{corke2011robotics}).
\Citeauthor{lowe1999recognition} kommt experimentell zu dem Schluss, dass die (analytisch) exakte Wahl von $k$ für die Lokalisierung der Merkmale und damit die Stabilität der Erkennung nur eine untergeordnete Rolle spielt. \Cref{fig:log-vs-dog} zeigt einen Vergleich der beiden Verfahren.

\section{Berechnung des Skalenraumes}
\label{subsec:scale_space_sift}

Die Berechnung der Skalenbilder wird unterteilt in $n_{oct}$ Oktaven, sowie einer Anzahl $n_{spo}$ Skalen pro Oktave.
Ausgehend von einer dem Ursprungsbild intrinsischen Grundglättung $\sigma_{min}$ wird nun für alle $s=1,\dots,n_{spo}$ je Oktave das Ursprungsbild $I$ mit dem Vielfachen $k^s \sigma_{min}$ geglättet, 
wobei $k$ eine noch festzulegende Konstante darstellt (z.B. \cref{eq:k_by_s}).

Das Unterteilen der Skalenberechnung in Oktaven begründet sich in dem in den \cref{fig:gaussian_sigma_delta,fig:gaussian_sigma_delta-3d} verdeutlichten Sachverhalt,
dass die relative Unschärfe eines mit  $\sigma$ weichgezeichneten Bildes stets in Bezug zur Interpixeldistanz $\delta$ der betrachteten Skale des Bildes (und damit dessen Fläche) steht; spezifisch:
ein Paar ${\sigma,\delta}$ einer Skale entspricht einem kleineren $\hat\sigma = \frac{1}{\lambda} \sigma$ mit antiproportional größerer Interpixeldistanz $\hat\delta = \lambda \delta$.
Sei $S_\delta$ ein linearer Sampling-Operator, der das Bild in $\delta$-Schritten abtastet und $S_2$ folglich eine Verkleinerung des Bildes um Faktor $2$ (die Bildfläche wird geviertelt), so gilt
%
\begin{align}
\mat G_{Gauss}(\frac{1}{\delta} \sigma) \ast \fun S_\delta \mat I \Longleftrightarrow \fun S_\delta \parens{ \mat G_{Gauss}(\sigma) \ast \mat I }
\end{align}
%
Durch die Reduktion der Bildfläche um Faktor $\delta^2$ wird dabei der erforderliche Rechenaufwand für die Verarbeitung mit jeder Oktave reduziert (\cite{lowe2004distinctive,ipol.2014.82}, vgl. \cite{shah:online:2012}).

\todo[inline]{sanity check}

\Cref{fig:sift_scales} verdeutlicht die Vorgehensweise von \algo{SIFT} an einem Beispiel mit $S=5$ Skalen pro Oktave, sowie insgesamt $O=3$ Oktaven.

Da durch die initiale Glättung des Bildes $\mat I$ mit $\sigma_0$ bereits Informationen in den hohen Frequenzen des Bildes verworfen werden,
wird das Eingangsbild zunächst in der Größe verdoppelt und mit einem (nicht näher spezifizierten) kleinen $\sigma$ geglättet. 
\Cite{lowe1999recognition} zeigt, dass hierdurch die Anzahl der gefundenen, über alle Skalen stabilen Merkmalspunkte vervierfacht werden kann, 
wobei eine weitere Vergrößerung des Bildes keinen signifikanten Einfluss hat.

Der Einfluss von Werten außerhalb $\mu \pm 3\sigma$ kann für die Betrachtung des Gaußoperators vernachlässigt werden (\cite[176]{sonka2008image}, vgl. \cref{fig:gaussian_at_sigma}); 
um einen zu starken Beschnitt der Werte in den Randbereichen des Kernes zu vermeiden, 
wählt z.B. \cite{Nguyen:2007:GG:1407436} einen Filterradius ("`support"') von $\floor*{3\sigma}$ je Dimension.
Diese grundsätzliche Abhängigkeit der Filtergröße von seiner Parametrierung führt jedoch (ungeachtet der Ausnutzung der Separierbarkeit) im vorliegenden Fall zu einem ebenfalls exponential steigenden Rechenaufwand, 
weswegen \cite{lowe1999recognition} einen Kern einer festen Breite von $7$ Stützpunkten (d.h. einen Radius von drei Werten in jeder Dimension) bei einer Glättung von $\sigma=\sqrt{2}$ verwendet.

Eine Besonderheit der der Gaußfunktion $\fun g(x | \mu, \sigma)$ ist, dass sowohl ihr Produkt, als auch ihre Faltung wieder eine Gaußfunktion ergeben\todo{Quelle}.
Bei der Faltung zweier Gaußfunktionen unterschiedlicher Standardabweichungen $\sigma_a, \sigma_b$ gilt bezüglich der Standardabweichung $\sigma_c$ des Ergebnisses
%
\begin{align}
\fun g(x | \sigma_c) &= \fun g(x | \sigma_a ) \ast \fun g(x |  \sigma_b ) \label{eq:convolution_twosigma} \\
\sigma_c &= \sqrt{ (\sigma_a )^2 + ( \sigma_b )^2 } \label{eq:convolution_twosigma_sqrt} 
\end{align}
\todo{Ziehen aus zwei Normalverteilungen}
%
Konkret bedeutet \cref{eq:convolution_twosigma_sqrt}, 
dass die wiederholte Anwendung desselben Gaußoperators auf ein Bild identisch ist mit der direkten Anwendung eines mit $\sigma' = \sqrt{2}\sigma$ "`breiteren"' Operators auf das Ursprungsbild, 
wobei sich das exponentielle Wachstum in $k^p\sigma_0$ nun bezüglich der Filtergröße als prinzipiell ungünstig herausstellt.

Abhilfe schafft es, anstatt der Filterung
%
\begin{align}
\mat L_p &= \mat G_{Gauss}(k^p \, \sigma) \ast \mat I
\end{align}
%
rekursiv vorzugehen, wobei analog \cref{eq:convolution_twosigma_sqrt} gilt:
%
\begin{align}
\sigma_{p+1} &= \sqrt{ \sigma^2 + \sigma^2_{p} } \label{eq:sift_convolution_twosigma_sqrt} 
\end{align}
%
Über die Identität
%
\begin{align}
\begin{split}
\sigma_{p+1} &= k^p \, \sigma_0 \\ 
			 &= k \left( k^{p-1} \sigma_0 \right) \\		 
			 &= k \, \sigma_p \\
\end{split} \label{eq:sift_sigman}
\end{align}
%
ergibt sich gemäß \cref{eq:sift_convolution_twosigma_sqrt} der Zusammenhang
%
\begin{align}
k^{p+1}\sigma_1 &= \sqrt{ \sigma^2 + \parens{k^p\sigma_0}^2 } \\
\sigma &= \sqrt{ \parens{k^{p+1}\sigma_0}^2 - \parens{k^p\sigma_0}^2 } \label{eq:sift_deconv} \\
       &= k^p\sigma_0 \, \sqrt{ k^2-1 } \label{eq:relative_sift_sigma}
\end{align}
%
wodurch sich für alle $k=1$ (d.h. ein Vervielfachen der Standardabweichung um $1$ in jedem Schritt) wie erwartet die Identität mit $\sigma=0$ ergibt, 
sowie für alle $1<k<\sqrt{2}$ eine Reduktion der notwendigen Standardabweichung erreicht wird.
Die Funktion $\sqrt{k^2+1}$ erreicht bei $k=\sqrt{2}$ den Wert $1$,
wodurch für höhere $k$ entsprechend größere Filter benötigt werden. Eine Beispielimplementierung in \MATLAB findet sich in \cref{lst:matlab_scalespace_sigma}.
\Cite{lowe2004distinctive} definiert $k$ als eine Funktion der Anzahl der geforderten Skalen $s$ als
%
\begin{align}
k &= 2^{\sfrac{1}{s}} \label{eq:k_by_s}
\end{align}
%
so dass für alle $s>2$ ein wünschenswerter Effekt eintritt, während für $s=2$ die Wurzel in \cref{eq:relative_sift_sigma} verschwindet\todo{Verweis auf notwendige Anzahl der Skalen für DoG-Extremafindung}.
\Citeauthor{lowe2004distinctive} geht weiterhin davon aus, dass jedes Originalbild $I$ bereits als mit $\sigma_n=0.5$ geglättet angenommen werden kann\todo{aber wieso?}, weswegen das Erzeugen des Skalenraumes anstelle von
%
\begin{align}
\mat L_{p+1} &= \mat G_{Gauss}(\sigma) \ast \mat L_p
\end{align}
%
praktisch mit
%
\begin{align}
\mat L_{p+1} &= \mat G_{Gauss}(\sqrt{\sigma^2 - \sigma_n^2}) \ast \mat L_p
\end{align}
%
durchgeführt wird (\cite{vlfeat:online:2016}, vgl. \cref{eq:sift_deconv}), wodurch die Filtergröße -- mit ihr der benötigte Rechenaufwand -- zusätzlich reduziert wird.

\todo[inline]{Hinweis auf relative Pixeldichte bei Gauss - anatomy of SIFT benennt die Standardabweichung konkreter}
\todo[inline]{Paper ausbuddeln, das auf Probleme bei \textit{falsch} angenommener Grundglättung hinweist - ausprobieren!}
\todo[inline]{EXIF-Daten der Handybilder speichern, um über Modelle Informationen über Grundglättung zu gewinnen}

\begin{lstlisting}[style=matlab,caption={Ermittlung des $\sigma$ zur Skalenraumerstellung}, label=lst:matlab_scalespace_sigma]
s                     = 3;
k                     = 2^(1/s);
sigma_0               = 1.6;
S                     = s+3;

sigma_current         = sigma_0;
sigma_total(1)        = sigma_current;

for p=0:(S-1)
    % Direkte Ermittlung von /*@$\sigma_{p+1}$@*/ gemäß Gleichung /*@\labelcref{eq:sift_sigman}@*/
    sigma_direct      = k^p * sigma_0;
    
    % Relative Ermittlung von /*@$\sigma_{p+1}$@*/ auf Basis von /*@$\sigma_{p}$@*/ gemäß Gleichung /*@\labelcref{eq:relative_sift_sigma}@*/
    sigma             = k^p * sigma_0 * sqrt( k^2 - 1 );
    sigma_next        = sqrt( sigma^2 + sigma_current^2 );

    direct_sigma(p+1) = sigma_direct;
    sigma_total(p+1)  = sigma_current;
    relative_sigma(p+1) = sigma;
    sigma_current = sigma_next;
end

direct_sigma, sigma_total, relative_sigma
\end{lstlisting}

\begin{figure}
	\centering
	\begin{subfigure}[b]{\textwidth}
		\centering
		\includestandalone{kernels/laplacian-of-gaussian}
		\caption{LoG: Erste und zweite Ableitung einer Gaußfunktion mit $\mu=0, \sigma=1.6$.}
		\label{fig:log}
	\end{subfigure}	

	\begin{subfigure}[b]{\textwidth}
		\centering
		\includestandalone{kernels/difference-of-gaussian}
		\caption{DoG: Differenz zweier Gaußfunktionen $g(x)$ mit $\mu=0$ und  unterschiedlichem $\sigma$ im Verhältnis $1:1.6$.}
		\label{fig:dog}
	\end{subfigure}
	
	\begin{subfigure}[b]{\textwidth}
		\centering
		\includestandalone{kernels/log-dog-error}
		\caption{Fehler der DoG-Approximation bei $\sigma$-Verhältnissen von $1:1.6$ ($\sigma_{D}=1.0$), $1:1.3$ ($\sigma_{D}=1.2$) und $1:1.23$ ($\sigma_{D}=1.3$).}
	\end{subfigure}
	
	\caption{Laplacian-of-Gaussian (LoG) und Difference-of-Gaussian (DoG)}
	\label{fig:log-vs-dog}
\end{figure}

\begin{figure}
	\centering
	\includestandalone{kernels/gaussian-at-sigma}
	\caption{Werte des Gaußkernes in Entfernungen von $k\sigma$}
	\caption*{Die Wahrscheinlichkeitsdichtefunktion der Normalverteilung $g_{Gauss}(x,\sigma|\mu=0)$ evaluiert an $x=k\sigma$ für variable $\sigma$. Werte kleine $10^0$ stellen Abschwächungen des Signales dar. Für $k=3, \sigma=1$ stellt sich bereits eine Abschwächung um mehr als $10^{-2}$ ein; bei einer Abschwächung um ca. $2\cdot 10^{-3}$ sind \SI{8}{\bit}-Intensitätswerte durch Diskretisierungseffekte vollständig unterdrückt.}
	\label{fig:gaussian_at_sigma}
\end{figure}

\begin{figure}
	\centering
	\includestandalone{kernels/gaussian-sigma-delta}
	\caption{Zusammenhang von Standardabweichung $\sigma$ und Interpixeldistanz $\delta$}
	\caption*{Normierte Varianten des eindimensionalen Gaußfilters mit unterschiedlichen $\sigma$ über unterschiedlichen Interpixeldistanzen $\delta$. Dieselben Funktionswerte sind für die jeweiligen Interpixeldistanzen $\delta=1$ (Vierecke), $\delta=0.5$ (Dreiecke) und $\delta=2$ (Kreise) markiert. Wird die Interpixeldistanz halbiert (das Bild um Faktor zwei vergrößert), muss die Standardabweichung verdoppelt werden, um dieselbe Filterwirkung zu erzielen; siehe auch \cref{fig:gaussian_sigma_delta-3d}.}
	\label{fig:gaussian_sigma_delta}
\end{figure}

\pgfplotsset{
colormap={whitered}{color(0cm)=(white); color(1cm)=(black!75!blue)}
}

\begin{figure}
	\centering
	\includestandalone{kernels/gaussian-sigma-delta-3d}
	
	\caption{Zweidimensionaler Gaußfilter über Pixelraster}
	\caption*{Zweidimensionaler Gaußfilter mit Standardabweichung $\sigma_x=\sigma_y=1$ und $\mu=0$. Die Wahl von $\sigma$ ist von der Interpixeldistanz $\delta$ abhängig (hier $\delta=1$); in Bezug auf das überlagerte dunklere Raster ($\delta=0.5$, vgl. \cref{fig:gaussian_sigma_delta}) wird eine geringere Fläche abgedeckt, die Wirkung der Filterung ist schwächer.}
	\label{fig:gaussian_sigma_delta-3d}
\end{figure}

\section{Merkmalserkennung im Skalenraum}

Soll die Merkmalserkennung in $S$ "`Skalen"' durchgeführt werden,
wird eine tatsächliche Anzahl von $S+3$ Skalen benötigt.
Der aus \cite{lowe2004distinctive} nicht schlüssig hervorgehende Sachverhalt liegt in der eingehend erwähnten dreidimensionalen Extremasuche begründet:
Für die Durchsuchung eines \DoG-Bildes ("`Skale"') werden je zwei umliegende \DoG-Bilder benötigt, 
für deren Berechnung wiederum jeweils zwei Skalen der Oktave vorliegen müssen; in je vier Skalen kann demnach \textit{eine} Detektion durchgeführt werden, 
wobei sich zwei "`benachbarte"' Suchen je drei Skalen teilen. \Cref{fig:sift_scales} macht dies deutlich.

\begin{figure}
	\centering
	\includestandalone{sift-scales/sift-scales}
	\caption{Extremadetektion im Scale-Space mittels Difference-of-Gaussian}
	\caption*{\algo{SIFT}-Featuredetektion im Skalenraum: $s=2$, $\sigma_0=\sigma_{min}=0.5\sqrt{2}$, $\sigma_n=0$, $k=2^{\sfrac{1}{s}}=\sqrt{2}$. Drei Oktaven zu je $s+3=5$ Skalen; Aus je zwei Skalen wird ein DoG-Bild generiert, in je drei DoG-Bildern werden dreidimensionale Extrema detektiert. Ein Pixel, das ein Extremum in der lokalen, sowie der darüber- und darunterliegenden Skale ist, ist ein \textit{keypoint}.}
	\label{fig:sift_scales}
\end{figure}

Die Erkennung selbst ist nun trivial:
Für jedes durchsuchte Pixel $p_{x,y,\sigma}$ mit $\sigma=k^s\sigma_0$ wird entschieden, ob es ein Maximum oder Minimum seiner $\num{3}\times \SI{3}{\px}$-Nachbarschaft ist, wozu maximal $8$ Vergleiche notwendig sind.
Ist dies nicht der Fall, wird der Test abgebrochen und mit dem nächsten Pixel fortgefahren; ist das Pixel ein Extremum seiner Umgebung, wird die Suche auf die darunterliegende Umgebung um $p_{x,y,\sigma_-}$ mit $\sigma_-=k^{s-1}\sigma_0$  ausgedehnt%
\footnote{Da Extrema einer Skale auch immer in den darunterliegenden Skalen zu finden sind, ist dies eine Optimierung der Suche.}
, was zu maximal~$9$ zusätzlichen Tests führt.
Ist $p_{x,y,\sigma}$ auch hier ein Extremum, wird die Prüfung entsprechend in $p_{x,y,\sigma_+}$ mit $\sigma_+=k^{s+1}\sigma_0$ wiederholt.
Pixel, die das Extremum ihrer $\SI{28}{\px}$-Umgebung sind, sind ein Merkmal in $\braces{x,y,k^s\sigma_0}$ und werden als \textit{keypoint} registriert.

\clearpage
\pagenumbering{Roman}
\appendix

\chapter{Algorithmen und Abbildungen}

\section{Energy-Based Retargeting}

\begin{lstlisting}[style=matlab,caption={Ermittelung des Schwellenwertes für den Energiebeschnitt}, label=lst:matlab_energy_threshold]
function [ t, m, s, border_energies ] = energyThreshold( J, subdivision )
    if ~exist('subdivision', 'var')
        subdivision = 5;
    end

    [M, N]   = size(J);
    height   = floor(M/subdivision);
    width    = floor(N/subdivision);
    
    J_top    = J(1:height,:);
    J_bottom = J(M-height:M,:);
    J_left   = J(height:M-height-1, 1:width);
    J_right  = J(height:M-height-1, N-width-1:N);

    border_energies = [ reshape(J_top,1,[]),  reshape(J_bottom,1,[]), ...
                        reshape(J_left,1,[]), reshape(J_right,1,[]) ];

    m = median(border_energies);
    s = std(border_energies);
    t = m + 4*s;
end
\end{lstlisting}

\begin{lstlisting}[style=matlab,caption={Ermittelung der Bounding Box anhand des Schwellwertes}, label=lst:matlab_energy_box]
function [ box ] = findBoundingBox( I, threshold, level, offset )
    [M, N] = size(I);
    top  = 1 + offset;  left = 1 + offset;
    right = N - offset; bottom = M - offset;
    
    % Scan "von oben nach unten" über alle Zeilen und Spalten
    y_top = NaN;
    for y=top:bottom
        for x=left:right
            if abs(I(y,x)-level) < threshold;   continue;   end;
            y_top = y;
            break;
        end
        if ~isnan(y_top);   break;  end;
    end
    
    % Fehler: Alle Intensitäten liegen innerhalb des Schwellwertes /*@$\tau$@*/
    if isnan(y_top)
        box = [ 1, 1, M, N ];
        return;
    end
    
    % Scan "von unten nach oben" bis /*@$y_{top}$@*/ über alle Spalten
    y_bottom = NaN;
    for y=bottom:-1:y_top
        for x=left:right
            if abs(I(y,x)-level) < threshold;   continue;   end;
            y_bottom = y;
            break;
        end
        if ~isnan(y_bottom);    break;  end;
    end
    
    % Scan "von links nach rechts" im Bereich /*@$y_{top} \dots y_{bottom}$@*/
    x_left = right;
    for y=y_top:y_bottom
        x = left;
        while x <= x_left
            if abs(I(y,x)-level) >= threshold;
                x_left = x;
            end
            
            x = x+1;
            if x_left == left;  break;  end;
        end
        if x_left == left;  break;  end;
    end
    
    % Scan "von rechts nach links" bis /*@$x_{left}$@*/ im Bereich /*@$y_{top} \dots y_{bottom}$@*/
    x_right = left;
    for y=y_top:y_bottom
        x = right;
        while x >= x_right
            if abs(I(y,x)-level) >= threshold
                x_right = x;
            end
            
            x = x-1;
            if x_right == right;    break;  end;
        end
        if x_right == right;    break;  end;
    end
    
    box = [x_left, y_top, x_right, y_bottom];
end
\end{lstlisting}

\begin{lstlisting}[style=matlab,caption={Generierung des synthetischen Testbildes für den Energiebeschnitt}, label=lst:matlab_energy_generate_test]
M = 256;                    % Kantenlänge
N = M;
I = ones(M, N);             % /*@\cref{fig:energycrop_simple}@*/
%I = cumsum(ones(N)/N, 2);  % /*@\cref{fig:energycrop_cumsum}@*/
%I = toeplitz(1:N)/N;       % /*@\cref{fig:energycrop_toeplitz}@*/
I(32:224, 32:224) = 0.9375; % hellgraue Box
I(64:192, 32:224) = 0.875;  % hellgraue Box
I(64:192, 32:224) = 0.875;  % hellgraue Box
I(64:192, 64:192) = 0.75;   % hellgraue Box
I(96:160, 64:192) = 0.5;    % hellgraue Box
I(96:160, 96:160) = 0;      % schwarze Box

E = 0.05*randn(N);          % zero-mean noise mit /*@$\sigma=0.05$@*/
Ie = abs(I + E);            % Anwendung des Rauschens
Ie(Ie>1) = 1;
Ie(Ie<0) = 0;
\end{lstlisting}

\begin{lstlisting}[style=matlab,caption={Ermittelung des Energiebildes}, label=lst:matlab_energy_generate_energy]
Gx = conv2(Ie, 0.125*[-1 0 1; -2 0 2; -1 0 1]);
Gy = conv2(Ie, 0.125*[-1 -2 -1; 0 0 0; 1 2 1]);
J = abs(Gx) + abs(Gy);
J = J(1+2:M, 1+2:N);
\end{lstlisting}

\begin{lstlisting}[style=matlab,caption={Ermittelung der Beschnittbereiche}, label=lst:matlab_energy_generate_energy]
% Intensitätsbeschnitt
[~,m,s] = energyThreshold(Ie, 6);          % /*@\cref{lst:matlab_energy_threshold}@*/
tI      = sqrt(s);
vI      = m;
boxI    = findBoundingBox(Ie, tI, vI, 0)   % /*@\cref{lst:matlab_energy_box}@*/

% Energiebeschnitt
J       = (J-min(J(:)))/(max(J(:))-min(J(:)));
tJ      = energyThreshold(J);
vJ      = 0;
boxJ    = findBoundingBox(J, tJ, vJ, 0)
\end{lstlisting}

\clearpage

\begin{figure}
	\centering
	\begin{subfigure}[b]{0.27\textwidth}
		\begin{tikzpicture}
			\node[inner sep=0pt] (img) at (0,0)
		    	{\includegraphics[width=\textwidth]{entropy_crop/real/input}};
			\draw[-,dotted] (img.north west) -- (img.north east);
			\draw[-,dotted] (img.north west) -- (img.south west);
			\draw[-,dotted] (img.south west) -- (img.south east);
			\draw[-,dotted] (img.south east) -- (img.north east);
		\end{tikzpicture}
        \caption{$\mat I$}
    \end{subfigure}
    \qquad
    \begin{subfigure}[b]{0.27\textwidth}
		\begin{tikzpicture}
			\node[inner sep=0pt] (img) at (0,0)
		    	{\includegraphics[width=\textwidth]{entropy_crop/real/energy-inv}};   	
			\draw[-,dotted] (img.north west) -- (img.north east);
			\draw[-,dotted] (img.north west) -- (img.south west);
			\draw[-,dotted] (img.south west) -- (img.south east);
			\draw[-,dotted] (img.south east) -- (img.north east);    	
		\end{tikzpicture}
        \caption{$1 - \mat J$}
    \end{subfigure}
    
    \begin{subfigure}[b]{0.27\textwidth}
	\begin{tikzpicture}
    		\node[inner sep=0pt] (img) at (0,0)
		    	{\includegraphics[width=\textwidth]{entropy_crop/real/input}};
    
			\draw[-,dotted] (img.north west) -- (img.north east);
			\draw[-,dotted] (img.north west) -- (img.south west);
			\draw[-,dotted] (img.south west) -- (img.south east);
			\draw[-,dotted] (img.south east) -- (img.north east);

			\coordinate (TL) at (img.north west);
			\coordinate (BL) at (img.south west);
			\coordinate (TR) at (img.north east);
			\coordinate (BR) at (img.south east);

			\coordinate (YUL) at ($(TL)!138/1198!(BL)$);
			\coordinate (YUR) at ($(TR)!138/1198!(BR)$);
			\draw[-,red] (YUL) -- (YUR);

			\coordinate (YBL) at ($(TL)!1063/1198!(BL)$);
			\coordinate (YBR) at ($(TR)!1063/1198!(BR)$);
			\draw[-,red] (YBL) -- (YBR);
			
			\coordinate (XLT) at ($(TL)!123/1198!(TR)$);
			\coordinate (XLB) at ($(BL)!123/1198!(BR)$);
			\draw[-,red] (XLT) -- (XLB);

			\coordinate (XRT) at ($(TL)!992/1198!(TR)$);
			\coordinate (XRB) at ($(BL)!992/1198!(BR)$);
			\draw[-,red] (XRT) -- (XRB);
    	
		\end{tikzpicture}
        \caption{$C(\mat I, \tau{=}0.157, \mu{=}1)$}
    \end{subfigure}
    \qquad
    \begin{subfigure}[b]{0.27\textwidth}
		\begin{tikzpicture}
    		\node[inner sep=0pt] (img) at (0,0)
		    	{\includegraphics[width=\textwidth]{entropy_crop/real/input}};
    
			\draw[-,dotted] (img.north west) -- (img.north east);
			\draw[-,dotted] (img.north west) -- (img.south west);
			\draw[-,dotted] (img.south west) -- (img.south east);
			\draw[-,dotted] (img.south east) -- (img.north east);

			\coordinate (TL) at (img.north west);
			\coordinate (BL) at (img.south west);
			\coordinate (TR) at (img.north east);
			\coordinate (BR) at (img.south east);

			\coordinate (YUL) at ($(TL)!135/1198!(BL)$);
			\coordinate (YUR) at ($(TR)!135/1198!(BR)$);
			\draw[-,red] (YUL) -- (YUR);

			\coordinate (YBL) at ($(TL)!1073/1198!(BL)$);
			\coordinate (YBR) at ($(TR)!1073/1198!(BR)$);
			\draw[-,red] (YBL) -- (YBR);
			
			\coordinate (XLT) at ($(TL)!121/1198!(TR)$);
			\coordinate (XLB) at ($(BL)!121/1198!(BR)$);
			\draw[-,red] (XLT) -- (XLB);

			\coordinate (XRT) at ($(TL)!993/1198!(TR)$);
			\coordinate (XRB) at ($(BL)!993/1198!(BR)$);
			\draw[-,red] (XRT) -- (XRB);
    	
		\end{tikzpicture}
        \caption{$C(\mat J, \tau{=}0.207, \mu{=}0)$}
    \end{subfigure}
           
    \caption{Intensitäts- und Energieretargeting in einem Produktbild}
    \caption*{Energiewerte $\mat J$ normalisiert (invertiert für bessere Anzeige). Toleranz $\tau$ und Offset $\mu$ ermittelt in den Randregionen des jeweiligen Eingangsbildes $\mat I$ und $\mat J$.
    Beide Algorithmen beschneiden die Reflektion auf dem Untergrund,
    bildwichtige Details bleiben erhalten.}
    \label{fig:energycrop_real}
\end{figure}

\begin{figure}
	\centering
	\begin{subfigure}[b]{0.27\textwidth}
		\begin{tikzpicture}
			\node[inner sep=0pt] (img) at (0,0)
		    	{\includegraphics[width=\textwidth]{entropy_crop/simple/input}};
			\draw[-,dotted] (img.north west) -- (img.north east);
			\draw[-,dotted] (img.north west) -- (img.south west);
			\draw[-,dotted] (img.south west) -- (img.south east);
			\draw[-,dotted] (img.south east) -- (img.north east);
		\end{tikzpicture}
        \caption{$\mat I$}
    \end{subfigure}
    \qquad
    \begin{subfigure}[b]{0.27\textwidth}
		\begin{tikzpicture}
			\node[inner sep=0pt] (img) at (0,0)
		    	{\includegraphics[width=\textwidth]{entropy_crop/simple/energy-inv}};   	
			\draw[-,dotted] (img.north west) -- (img.north east);
			\draw[-,dotted] (img.north west) -- (img.south west);
			\draw[-,dotted] (img.south west) -- (img.south east);
			\draw[-,dotted] (img.south east) -- (img.north east);    	
		\end{tikzpicture}
        \caption{$1 - \mat J$}
    \end{subfigure}
    
    \begin{subfigure}[b]{0.27\textwidth}
	\begin{tikzpicture}
    		\node[inner sep=0pt] (img) at (0,0)
		    	{\includegraphics[width=\textwidth]{entropy_crop/simple/input}};
    
			\draw[-,dotted] (img.north west) -- (img.north east);
			\draw[-,dotted] (img.north west) -- (img.south west);
			\draw[-,dotted] (img.south west) -- (img.south east);
			\draw[-,dotted] (img.south east) -- (img.north east);

			\coordinate (TL) at (img.north west);
			\coordinate (BL) at (img.south west);
			\coordinate (TR) at (img.north east);
			\coordinate (BR) at (img.south east);

			\coordinate (YUL) at ($(TL)!64/254!(BL)$);
			\coordinate (YUR) at ($(TR)!64/254!(BR)$);
			\draw[-,red] (YUL) -- (YUR);

			\coordinate (YBL) at ($(TL)!214/254!(BL)$);
			\coordinate (YBR) at ($(TR)!214/254!(BR)$);
			\draw[-,red] (YBL) -- (YBR);
			
			\coordinate (XLT) at ($(TL)!32/254!(TR)$);
			\coordinate (XLB) at ($(BL)!32/254!(BR)$);
			\draw[-,red] (XLT) -- (XLB);

			\coordinate (XRT) at ($(TL)!224/254!(TR)$);
			\coordinate (XRB) at ($(BL)!224/254!(BR)$);
			\draw[-,red] (XRT) -- (XRB);
    	
		\end{tikzpicture}
        \caption{$C(\mat I, \tau{=}0.046, \mu{=}0.986)$}
    \end{subfigure}
    \qquad
    \begin{subfigure}[b]{0.27\textwidth}
		\begin{tikzpicture}
    		\node[inner sep=0pt] (img) at (0,0)
		    	{\includegraphics[width=\textwidth]{entropy_crop/simple/input}};
    
			\draw[-,dotted] (img.north west) -- (img.north east);
			\draw[-,dotted] (img.north west) -- (img.south west);
			\draw[-,dotted] (img.south west) -- (img.south east);
			\draw[-,dotted] (img.south east) -- (img.north east);

			\coordinate (TL) at (img.north west);
			\coordinate (BL) at (img.south west);
			\coordinate (TR) at (img.north east);
			\coordinate (BR) at (img.south east);

			\coordinate (YUL) at ($(TL)!30/254!(BL)$);
			\coordinate (YUR) at ($(TR)!30/254!(BR)$);
			\draw[-,red] (YUL) -- (YUR);

			\coordinate (YBL) at ($(TL)!223/254!(BL)$);
			\coordinate (YBR) at ($(TR)!223/254!(BR)$);
			\draw[-,red] (YBL) -- (YBR);
			
			\coordinate (XLT) at ($(TL)!30/254!(TR)$);
			\coordinate (XLB) at ($(BL)!30/254!(BR)$);
			\draw[-,red] (XLT) -- (XLB);

			\coordinate (XRT) at ($(TL)!224/254!(TR)$);
			\coordinate (XRB) at ($(BL)!224/254!(BR)$);
			\draw[-,red] (XRT) -- (XRB);
    	
		\end{tikzpicture}
        \caption{$C(\mat J, \tau{=}0.154, \mu{=}0)$}
    \end{subfigure}
           
    \caption{Intensitäts- und Energieretargeting auf homogenem Untergrund}
    \caption*{Analog \cref{fig:energycrop_real}.
    Beide Algorithmen liefern vergleichbare Ergebnisse und hängen stark
    von der Natur der Störüberlagerung ab. Gewinnung von $\mu,\tau$ gemäß \cref{lst:matlab_energy_generate_energy}.}
    \label{fig:energycrop_simple}
\end{figure}



\begin{figure}
	\centering
	\begin{subfigure}[b]{0.27\textwidth}
		\begin{tikzpicture}
			\node[inner sep=0pt] (img) at (0,0)
		    	{\includegraphics[width=\textwidth]{entropy_crop/cumsum/input}};
			\draw[-,dotted] (img.north west) -- (img.north east);
			\draw[-,dotted] (img.north west) -- (img.south west);
			\draw[-,dotted] (img.south west) -- (img.south east);
			\draw[-,dotted] (img.south east) -- (img.north east);
		\end{tikzpicture}
        \caption{$\mat I$}
    \end{subfigure}
    \qquad
    \begin{subfigure}[b]{0.27\textwidth}
		\begin{tikzpicture}
			\node[inner sep=0pt] (img) at (0,0)
		    	{\includegraphics[width=\textwidth]{entropy_crop/cumsum/energy-inv}};   	
			\draw[-,dotted] (img.north west) -- (img.north east);
			\draw[-,dotted] (img.north west) -- (img.south west);
			\draw[-,dotted] (img.south west) -- (img.south east);
			\draw[-,dotted] (img.south east) -- (img.north east);    	
		\end{tikzpicture}
        \caption{$1 - \mat J$}
        \label{fig:energycrop_cumsum_energy}
    \end{subfigure}
    
    \begin{subfigure}[b]{0.27\textwidth}
	\begin{tikzpicture}
    		\node[inner sep=0pt] (img) at (0,0)
		    	{\includegraphics[width=\textwidth]{entropy_crop/cumsum/input}};
    
			\draw[-,dotted] (img.north west) -- (img.north east);
			\draw[-,dotted] (img.north west) -- (img.south west);
			\draw[-,dotted] (img.south west) -- (img.south east);
			\draw[-,dotted] (img.south east) -- (img.north east);

			\coordinate (TL) at (img.north west);
			\coordinate (BL) at (img.south west);
			\coordinate (TR) at (img.north east);
			\coordinate (BR) at (img.south east);

			\coordinate (YUL) at ($(TL)!1/254!(BL)$);
			\coordinate (YUR) at ($(TR)!1/254!(BR)$);
			\draw[-,red] (YUL) -- (YUR);

			\coordinate (YBL) at ($(TL)!256/254!(BL)$);
			\coordinate (YBR) at ($(TR)!256/254!(BR)$);
			\draw[-,red] (YBL) -- (YBR);
			
			\coordinate (XLT) at ($(TL)!1/254!(TR)$);
			\coordinate (XLB) at ($(BL)!1/254!(BR)$);
			\draw[-,red] (XLT) -- (XLB);

			\coordinate (XRT) at ($(TL)!160/254!(TR)$);
			\coordinate (XRB) at ($(BL)!160/254!(BR)$);
			\draw[-,red] (XRT) -- (XRB);
    	
		\end{tikzpicture}
        \caption{$C(\mat I, \tau{=}0.360, \mu{=}0.76)$}
    \end{subfigure}
    \qquad
    \begin{subfigure}[b]{0.27\textwidth}
		\begin{tikzpicture}
    		\node[inner sep=0pt] (img) at (0,0)
		    	{\includegraphics[width=\textwidth]{entropy_crop/cumsum/input}};
    
			\draw[-,dotted] (img.north west) -- (img.north east);
			\draw[-,dotted] (img.north west) -- (img.south west);
			\draw[-,dotted] (img.south west) -- (img.south east);
			\draw[-,dotted] (img.south east) -- (img.north east);

			\coordinate (TL) at (img.north west);
			\coordinate (BL) at (img.south west);
			\coordinate (TR) at (img.north east);
			\coordinate (BR) at (img.south east);

			\coordinate (YUL) at ($(TL)!30/254!(BL)$);
			\coordinate (YUR) at ($(TR)!30/254!(BR)$);
			\draw[-,red] (YUL) -- (YUR);

			\coordinate (YBL) at ($(TL)!224/254!(BL)$);
			\coordinate (YBR) at ($(TR)!224/254!(BR)$);
			\draw[-,red] (YBL) -- (YBR);
			
			\coordinate (XLT) at ($(TL)!30/254!(TR)$);
			\coordinate (XLB) at ($(BL)!30/254!(BR)$);
			\draw[-,red] (XLT) -- (XLB);

			\coordinate (XRT) at ($(TL)!192/254!(TR)$);
			\coordinate (XRB) at ($(BL)!192/254!(BR)$);
			\draw[-,red] (XRT) -- (XRB);
    	
		\end{tikzpicture}
        \caption{$C(\mat J, \tau{=}0.373, \mu{=}0)$}
    \end{subfigure}
           
    \caption{Intensitäts- und Energieretargeting auf einfachem Intensitätsverlauf}
    \caption*{Analog \cref{fig:energycrop_real}.
    Die ermittelte Toleranz wirkt im Zusammenspiel mit Offset $\mu$. Durch seine differenzierende Natur ist der
    Energiebeschnitt invariant gegenüber (sanften) Verläufen, jedoch ebenfalls nicht gegenüber geringen Kontrasten.}
    \label{fig:energycrop_cumsum}
\end{figure}



\begin{figure}
	\centering
	\begin{subfigure}[b]{0.27\textwidth}
		\begin{tikzpicture}
			\node[inner sep=0pt] (img) at (0,0)
		    	{\includegraphics[width=\textwidth]{entropy_crop/toeplitz/input}};
			\draw[-,dotted] (img.north west) -- (img.north east);
			\draw[-,dotted] (img.north west) -- (img.south west);
			\draw[-,dotted] (img.south west) -- (img.south east);
			\draw[-,dotted] (img.south east) -- (img.north east);
		\end{tikzpicture}
        \caption{$\mat I$}
    \end{subfigure}
    \qquad
    \begin{subfigure}[b]{0.27\textwidth}
		\begin{tikzpicture}
			\node[inner sep=0pt] (img) at (0,0)
		    	{\includegraphics[width=\textwidth]{entropy_crop/toeplitz/energy-inv}};   	
			\draw[-,dotted] (img.north west) -- (img.north east);
			\draw[-,dotted] (img.north west) -- (img.south west);
			\draw[-,dotted] (img.south west) -- (img.south east);
			\draw[-,dotted] (img.south east) -- (img.north east);    	
		\end{tikzpicture}
        \caption{$1 - \mat J$}
        \label{fig:energycrop_toeplitz_energy}
    \end{subfigure}
    
    \begin{subfigure}[b]{0.27\textwidth}
	\begin{tikzpicture}
    		\node[inner sep=0pt] (img) at (0,0)
		    	{\includegraphics[width=\textwidth]{entropy_crop/toeplitz/input}};
    
			\draw[-,dotted] (img.north west) -- (img.north east);
			\draw[-,dotted] (img.north west) -- (img.south west);
			\draw[-,dotted] (img.south west) -- (img.south east);
			\draw[-,dotted] (img.south east) -- (img.north east);

			\coordinate (TL) at (img.north west);
			\coordinate (BL) at (img.south west);
			\coordinate (TR) at (img.north east);
			\coordinate (BR) at (img.south east);

			\coordinate (YUL) at ($(TL)!1/254!(BL)$);
			\coordinate (YUR) at ($(TR)!1/254!(BR)$);
			\draw[-,red] (YUL) -- (YUR);

			\coordinate (YBL) at ($(TL)!256/254!(BL)$);
			\coordinate (YBR) at ($(TR)!256/254!(BR)$);
			\draw[-,red] (YBL) -- (YBR);
			
			\coordinate (XLT) at ($(TL)!1/254!(TR)$);
			\coordinate (XLB) at ($(BL)!1/254!(BR)$);
			\draw[-,red] (XLT) -- (XLB);

			\coordinate (XRT) at ($(TL)!256/254!(TR)$);
			\coordinate (XRB) at ($(BL)!256/254!(BR)$);
			\draw[-,red] (XRT) -- (XRB);
    	
		\end{tikzpicture}
        \caption{$C(\mat I, \tau{=}0.302, \mu{=}0.569)$}
    \end{subfigure}
    \qquad
    \begin{subfigure}[b]{0.27\textwidth}
		\begin{tikzpicture}
    		\node[inner sep=0pt] (img) at (0,0)
		    	{\includegraphics[width=\textwidth]{entropy_crop/toeplitz/input}};
    
			\draw[-,dotted] (img.north west) -- (img.north east);
			\draw[-,dotted] (img.north west) -- (img.south west);
			\draw[-,dotted] (img.south west) -- (img.south east);
			\draw[-,dotted] (img.south east) -- (img.north east);

			\coordinate (TL) at (img.north west);
			\coordinate (BL) at (img.south west);
			\coordinate (TR) at (img.north east);
			\coordinate (BR) at (img.south east);

			\coordinate (YUL) at ($(TL)!30/254!(BL)$);
			\coordinate (YUR) at ($(TR)!30/254!(BR)$);
			\draw[-,red] (YUL) -- (YUR);

			\coordinate (YBL) at ($(TL)!224/254!(BL)$);
			\coordinate (YBR) at ($(TR)!224/254!(BR)$);
			\draw[-,red] (YBL) -- (YBR);
			
			\coordinate (XLT) at ($(TL)!30/254!(TR)$);
			\coordinate (XLB) at ($(BL)!30/254!(BR)$);
			\draw[-,red] (XLT) -- (XLB);

			\coordinate (XRT) at ($(TL)!224/254!(TR)$);
			\coordinate (XRB) at ($(BL)!224/254!(BR)$);
			\draw[-,red] (XRT) -- (XRB);
    	
		\end{tikzpicture}
        \caption{$C(\mat J, \tau{=}0.346, \mu{=}0)$}
    \end{subfigure}
           
    \caption{Intensitäts- und Energieretargeting auf komplexem Intensitätsverlauf}
    \caption*{Analog \cref{fig:energycrop_cumsum}.
    Durch den bidirektionalen Verlauf des Hintergrundes ist kein allgemeingültiger Intensitäts-Schwellenwert ermittelbar; der Energiebeschnitt ist invariant gegenüber Verläufen (vgl. \cref{fig:energycrop_cumsum_energy,fig:energycrop_toeplitz_energy}).}
    \label{fig:energycrop_toeplitz}
\end{figure}




\chapter{Softwarebibliotheken}

\section{MongoDB C++ Driver}

Für alle Builds kamen CMake 3.4 und Visual Studio 2015  unter Windows 10 zum Einsatz,
wobei für die Intel x64-Architektur kompiliert wurde.

Da die CMake-Konfigurationen der folgend aufgelisteten Bibliotheken hinsichtlich der Nomenklatur der Ausgabedateien keine Unterscheidung zwischen Release- und Debugbuilds durchführen, 
dies unter Windows jedoch aufgrund der unterschiedlichen Laufzeitumgebungen für Release- und Debugbuilds (\texttt{msvcp140.dll} und \texttt{msvcp140d.dll}) zu Problemen führte,
wurden separate Builds für beide Konfigurationen in unterschiedliche Zielordner ausgeführt.

\todo[inline]{Docker based builds}

\subsection{libbson}

\texttt{libbson} 1.3 wurde aus den git-Sourcen gebaut (\url{https://github.com/mongodb/libbson}, commit SHA \texttt{d5b0533f\-79855fc5\-d0393009\-ed902d2e\-d0b36336} vom 18. Januar 2016).
CMake-Builds dieses Stands werden fälschlicherweise als Version 1.0 ausgegeben.

\subsection{libmongoc}

\texttt{libmongoc} 1.3.2-dev ("`\texttt{mongo-driver-c}"') wurde aus den git-Sourcen gebaut (\url{https://github.com/mongodb/mongo-c-driver}, commit SHA \texttt{5f0720ed\-19cdeebf\-170e50fb\-ef39f5f6\-d81de6db} vom 27. Januar 2016). 
Hierzu wurde \texttt{libbson} dynamisch gelinkt (\texttt{bson-1.0.lib}).
CMake-Builds dieses Stands werden fälschlicherweise als Version 1.0 ausgegeben.

\subsection{mongo-cxx-driver}

\texttt{mongo-cxx-driver} 3.0.1-rc0 wurde aus den git-Sourcen gebaut (\url{https://github.com/mongodb/mongo-cxx-driver}, commit SHA \texttt{46bad97f\-4d908cd9\-e13222cc\-3d6ba6ca\-d5625e64} vom 24. Januar 2016). 
Der Build erfolgte gegen die obigen \texttt{libbson} und \texttt{libmongoc}, sowie Boost 1.60.0 (\url{http://www.boost.org/}, Release vom 17. Dezember 2015).

\section{OpenCV}

Für alle Builds kamen CMake 3.4 und Visual Studio 2015  unter Windows 10 zum Einsatz,
wobei für die Intel x64-Architektur kompiliert wurde.
Debug- und Releasebuilds wurden wie üblich erstellt.

\subsection{libopencv}

OpenCV 3.1.0 wurde aus den git-Sourcen gebaut (\url{https://github.com/Itseez/opencv}, commit SHA \texttt{92387b1e\-f8fad151\-96dd5f7f\-b4931444\-a68bc93a} vom 18. Dezember 2015).
"`Nonfree"'-Extras aus den Contributions (\cref{subsec:opencv_contrib}) für \algo{SIFT} und \algo{SURF}, sowie die Eigen-Library (\cref{subsec:eigen}) wurden manuell hinzugefügt.
Die integrierte Unterstützung für JPEG (\texttt{libjpeg} 90), PNG  (\texttt{libjng} 1.6.19), sowie WebP (\texttt{libwebp} 0.3.1) wurde verwendet.
Auf die Verwendendung der neueren \texttt{libwebp} 0.5.0 (Mirror unter \url{https://github.com/webmproject/libwebp}) wurde verzichtet, 
da bei der Verarbeitung der verwendeten WebP-Bilder keine Probleme auftraten.

\todo[inline]{Eigen ggf. wieder raus}
\todo[inline]{Performance mit neuerer libwep prüfen!}

\subsection{nonfree}
\label{subsec:opencv_contrib}

OpenCV Contributions 3.1.0 wurde aus den git-Sourcen gebaut (\url{https://github.com/Itseez/opencv_contrib}, commit SHA \texttt{5409d5ad\-560523c8\-5c6796cc\-5a009347\-072d883c} vom 17. Dezember 2015). 

\subsection{Eigen}
\label{subsec:eigen}

Eigen 3.2.92 wurde aus mercurial-Sourcen gebaut (Mirror \url{https://github.com/RLovelett/eigen}, commit SHA \texttt{762d0471\-b2add463\-1c0c68ca\-b601a3dd\-77a11d0e} vom 27. Januar 2016).

\todo[inline]{add}

\backmatter

\clearpage
\phantomsection
\addcontentsline{toc}{section}{Bildquellen}
\printbibliography[keyword=image,title={Bildquellen}]

\clearpage
\phantomsection
\addcontentsline{toc}{section}{Literatur}
\printbibliography[notkeyword=image]

\end{document}