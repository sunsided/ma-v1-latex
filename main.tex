\documentclass[a4paper, 11pt, oneside, english, german, ngerman]{thesis}

\usepackage{fancyhdr}
\usepackage{eurosym}
\usepackage{geometry}
\usepackage{lipsum}
\usepackage{graphicx}
\usepackage[figurewithin=section, 
		   font=small, 
		   labelfont=bf]
		   {caption}
\usepackage{subcaption}
\usepackage{wrapfig}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{inconsolata}
\usepackage{lmodern}
\usepackage[english,german,ngerman]{babel}
\usepackage{xspace}

\usepackage[super]{nth}
\usepackage{makeidx}

\usepackage{amsmath}
\usepackage{bm}
\usepackage{isomath}
\usepackage{xfrac}
\usepackage{cancel}

\usepackage{booktabs}

\usepackage{xcolor}
\usepackage{pgfplots}
\pgfplotsset{compat=1.11}

\usepackage{tikz}
\usetikzlibrary{calc}
\usetikzlibrary{fit}

\usepackage{varioref}
\usepackage[pdftex]{hyperref}
\usepackage{cleveref}
\usepackage[binary-units=true]{siunitx}
\usepackage{todonotes}

\usepackage[babel]{csquotes}
\usepackage[backend=biber,style=authoryear]{biblatex}

\usepackage{listings}

% color definitions for listings
\definecolor{bluekeywords}{rgb}{0.13,0.13,1}
\definecolor{greencomments}{rgb}{0,0.5,0}
\definecolor{redstrings}{rgb}{0.9,0,0}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\hypersetup{urlcolor=black, colorlinks=true}

\lstset{literate=
  {á}{{\'a}}1 {é}{{\'e}}1 {í}{{\'i}}1 {ó}{{\'o}}1 {ú}{{\'u}}1
  {Á}{{\'A}}1 {É}{{\'E}}1 {Í}{{\'I}}1 {Ó}{{\'O}}1 {Ú}{{\'U}}1
  {à}{{\`a}}1 {è}{{\`e}}1 {ì}{{\`i}}1 {ò}{{\`o}}1 {ù}{{\`u}}1
  {À}{{\`A}}1 {È}{{\'E}}1 {Ì}{{\`I}}1 {Ò}{{\`O}}1 {Ù}{{\`U}}1
  {ä}{{\"a}}1 {ë}{{\"e}}1 {ï}{{\"i}}1 {ö}{{\"o}}1 {ü}{{\"u}}1
  {Ä}{{\"A}}1 {Ë}{{\"E}}1 {Ï}{{\"I}}1 {Ö}{{\"O}}1 {Ü}{{\"U}}1
  {â}{{\^a}}1 {ê}{{\^e}}1 {î}{{\^i}}1 {ô}{{\^o}}1 {û}{{\^u}}1
  {Â}{{\^A}}1 {Ê}{{\^E}}1 {Î}{{\^I}}1 {Ô}{{\^O}}1 {Û}{{\^U}}1
  {œ}{{\oe}}1 {Œ}{{\OE}}1 {æ}{{\ae}}1 {Æ}{{\AE}}1 {ß}{{\ss}}1
  {ű}{{\H{u}}}1 {Ű}{{\H{U}}}1 {ő}{{\H{o}}}1 {Ő}{{\H{O}}}1
  {ç}{{\c c}}1 {Ç}{{\c C}}1 {ø}{{\o}}1 {å}{{\r a}}1 {Å}{{\r A}}1
  {€}{{\EUR}}1 {£}{{\pounds}}1
}

\newcommand{\listingpartial}{\mbox{$\partial$}}

%\lstdefinestyle{csharp}{
%	language=[Sharp]C,
%}

\lstdefinestyle{matlab}{
	language=matlab,
	showspaces=false,
	showtabs=false,
	breaklines=true,
	showstringspaces=false,
	breakatwhitespace=true,
	escapeinside={/*@}{@*/},
	backgroundcolor=\color{backcolour},  
	commentstyle=\color{greencomments},
	keywordstyle=\color{bluekeywords}\bfseries,
	stringstyle=\color{redstrings},
	basicstyle=\footnotesize\ttfamily,
	columns=fullflexible,
	tabsize=2,
	numbers=left,                    
    numbersep=5pt,
    numberblanklines=true,
    morekeywords={syms,pretty}
}

% cleveref bindings for other packages
\crefname{lstlisting}{listing}{listings}
\Crefname{lstlisting}{Listing}{Listings}

\bibliography{quellen.bib}

\geometry{a4paper,
		top=25mm, 
		left=40mm, 
		right=25mm, 
		bottom=30mm, 
		headsep=10mm, 
		footskip=12mm}

\graphicspath{ {./images/}{./images/source/} }

\pagestyle{plain}
\pagenumbering{arabic}

% Generate the index and glossary
\makeindex

\newcommand{\name}[1]{\textsc{#1}}
\newcommand{\algo}[1]{\textit{#1}}
\newcommand{\acro}[1]{\texttt{#1}}
\newcommand{\datatype}[1]{\texttt{#1}}
\newcommand{\imgfile}[1]{\texttt{#1}}
\newcommand{\engl}[1]{engl.: \textit{#1}}

\newcommand{\LAB}{L\textsuperscript{\tiny *}a\textsuperscript{\tiny *}b\textsuperscript{\tiny *}}
\newcommand{\LUV}{L\textsuperscript{\tiny *}u\textsuperscript{\tiny *}v\textsuperscript{\tiny *}}
\newcommand{\RGB}{RGB}
\newcommand{\HLS}{HLS}
\newcommand{\HSV}{HSV}

% math commands
\renewcommand{\vec}{\vectorsym}
\newcommand{\mat}{\matrixsym}
\newcommand{\fun}{\mathit}
\newcommand{\transp}{^{\mathstrut\scriptscriptstyle{\top}}}
\DeclareMathOperator*{\argmin}{\arg\!\min}
\newcommand{\thetavec}{\ensuremath{\vec{\theta}}}
\newcommand{\order}[1]{\ensuremath{\mathcal{O}(#1)}}
\newcommand{\mean}[1]{\ensuremath{\bar{#1}}}

% "SI" units
\sisetup{detect-weight=true, detect-family=true}
\DeclareSIUnit	\px{px}

\hypersetup{pdftitle={Large Scale Image Retrieval Using Features Extracted From Mobile Phone Camera Stills}}
\hypersetup{pdfsubject={Image Retrieval}} 
\hypersetup{pdfauthor={Markus Mayer}} 
\hypersetup{pdfkeywords={{Image Retrieval}, {CBIR}, {Image Matching}}}

\begin{titlepage}
	\title{\textbf{Large Scale Image Retrieval} \\ Using Features Extracted From \\ Mobile Phone Camera Stills}
	\author{Markus Mayer}
	\date{\today}
\end{titlepage}

\begin{document}

%Term definitions
%\newglossaryentry{sse}{name=SSE, description={Sum of Squared Errors}}

\maketitle

\begin{abstract}
Like regular \textit{Image Retrieval}, but on a larger scale.
\end{abstract}

\frontmatter
\pagenumbering{roman} 
\pagestyle{fancy}

\clearpage
\phantomsection
\addcontentsline{toc}{section}{Inhaltsverzeichnis}
\tableofcontents

\clearpage
\phantomsection
\addcontentsline{toc}{section}{Abbildungsverzeichnis}
\listoffigures

\clearpage
\phantomsection
\addcontentsline{toc}{section}{Tabellenverzeichnis}
\listoftables

\clearpage
\phantomsection
\addcontentsline{toc}{section}{Listings}
\lstlistoflistings

\mainmatter
\pagenumbering{arabic}
\pagestyle{fancy}

\chapter{Bildvorverarbeitung}

\section{Kanalreduktion und Graustufenumwandlung}

Während für einige Methoden der Bildanalyse und des Content-Based Image Retrieval Farbinformationen -- z.B. in Form von Farbhisto- und -korrelogrammen -- eine Rolle spielen, ist dies für andere Methoden -- etwa Scale-Invariant Feature Transforms oder Local Binary Patterns - nicht der Fall.
Eine Grundannahme ist hier, dass bildrelevante Merkmale weniger in Farb-, als in Intensitätsunterschieden zu finden sind, wie sie etwa durch Objektkanten hervorgerufen werden.
Die Transformation von unabhängigen Farbintensitätskanälen, wie sie aus der additiven (RGB-) oder subtraktiven (CMY-) Farbmischung bekannt sind, auf einen einzigen (idealerweise dekorrelierten) Intensitätskanal führt dabei ebenfalls zu einer Verringerung der notwendigen Verarbeitungszeit und Speicherkapazität, was insbesondere einer Echtzeit-Datenverarbeitung entgegenkommt.

\subsection{Graustufenumwandlung}
\label{subsec:graustufen}

\cite{kanan2012color} weisen darauf hin, dass die exakte Methode der Umwandlung von Farb- in Intensitätswerte oft nicht näher spezifiziert und als irrelevant bezüglich der weiteren Verarbeitung angenommen wird.
Sie vergleichen daher verschiedene wahrnehmungs- und nicht-wahrnehmungspsychologisch inspirierte Methoden der Konvertierung und überprüfen diese mittels der Extraktion der Featuredeskriptoren \algo{SIFT}, \algo{SURF}, \algo{Geometric Blur} und \algo{Local Binary Patterns} an verschiedenen Bilddatensätzen, u.a. kategorisierten Objekten in Caltech 101 (\cite{fei2007learning,caltech101:online}), Gesichtern des AR Face Dataset (\cite{martinez1998benavente}) und Objekten in variablen Ausleuchtungsszenarien in \cite{barnard2002data}. 
Hervorzuheben sind hierbei die Methoden \algo{Intensity}, \algo{Gleam}, \algo{Luminance}, \algo{Luma} und \algo{Lightness}, sowie \algo{Luster}.
Der Wertebereich wird als $0 \dots 1$ angenommen, wobei einige der Methoden sich der Gamma-Korrekturfunktion
%
\begin{align}
\Gamma(t) = t^{\sfrac{1}{\gamma}}
\end{align}
%
mit $\gamma = 2.2$ bedienen.
Diese Funktion überführt die lineare Farbintensität $t$ in eine nichtlineare, der menschlichen Helligkeitswahrnehmung ähnlicheren Abbildung. \todo{Quelle}

\algo{Intensity} und \algo{Gleam} entsprechen dem regulären und gamma-korrigierten Mittelwert der drei \RGB-Farbkanäle (vgl. \cref{fig:conv_intensity,fig:conv_gleam}):
%
\begin{align}
\mat I_{Intensity} &= \frac{1}{3}(\mat R + \mat G + \mat B) \\
\mat I_{Gleam} &= \frac{1}{3}(\Gamma(\mat R) + \Gamma(\mat G) + \Gamma(\mat B))
\end{align}

"`\algo{Luster}"' ist der \textit{Lightness}-Kanal des \HLS-Farbraumes (Hue, Lightness, Saturation) und entspricht dem Mittelwert der minimalen und maximalen \RGB-Kanalintensitäten
%
\begin{align}
\mat I_{Luster} &= \frac{1}{2} \left( \min (\mat R, \mat G, \mat B) + \max (\mat R, \mat G, \mat B) \right)
\end{align}
%
und entspricht einer ausreißertoleranteren Variante des Value-Kanals $\mat I_{Value} = \max (\mat R, \mat G, \mat B)$ des \HSV-Farbraumes (Hue, Saturation, Value); vgl. \cref{fig:conv_luster,fig:conv_value}.

Die Methoden \algo{Luminance} \todo{Quelle Pratt} und \algo{Luma} (vgl. \cref{fig:conv_luminance,fig:conv_luma}) arbeiten als gewichtete Summe der einzelnen Farbintensitäten, definiert als
%
\begin{align}
\mat I_{Luminance} &= 0.299 \mat R + 0.587 \mat G + 0.114 \mat B \\
\mat I_{Luma} &= 0.2126 \, \Gamma(\mat R) + 0.7152 \, \Gamma(\mat G) + 0.0722 \, \Gamma(\mat B)
\end{align}
%
wobei $\mat I_{Luma}$ einer gammakorrigierten Form von $\mat I_{Luminance}$ entspricht, die für High-Definition-TV-Geräte entwickelt wurde \todo{Quelle}.
\Cite{nixon2012feature} definieren $\mat I_{Luminance}$ derweil unterschiedlich als
%
\begin{align*}
\mat I_{Luminance} &= 0.18 \mat R + 0.79 \mat G + 0.02 \mat B
\end{align*}
%
Dieser Unterschied dürfte jedoch keinen maßgeblichen Einfluss auf das von \citeauthor{kanan2012color} ermittelte Ergebnis haben.

\algo{Lightness} schlussendlich entspricht der wahrnehmungspsychologisch gleichmäßigen Helligkeitsinformation $L^*$ der \LAB- und \LUV-Farbräume (CIE-LAB und CIE-LUV), definiert als
%
\begin{align}
\mat I_{Lightness} &= \frac{1}{100}\left(100 \, f(\mat Y) - 16\right)
\end{align}
%
mit
%
\begin{align*}
\mat Y &= 0.2126 \mat R + 0.7152 \mat G + 0.0722 \mat B \\
f(t) &= \begin{cases}
		t^{\sfrac{1}{3}} & \text{wenn} \quad t > (\sfrac{6}{29})^3 \\
		\frac{1}{3}\left(\frac{29}{6} \right)^2 t + \frac{4}{29} & \text{sonst}
	\end{cases}
\end{align*}
%
wobei der Wertebereich auf $0 \dots 1$ normiert wurde (vgl. \cref{fig:conv_lightness}).
Diese Farbräume dekorrelieren Helligkeits- von Farbinformationen, wobei die a\textsuperscript{\tiny *}- und b\textsuperscript{\tiny *}-, bzw. u\textsuperscript{\tiny *}- und v\textsuperscript{\tiny *}-Achsen komplementäre Farbinformationen abbilden (etwa rot-grün und glau-gelb). \todo{Quelle}

\todo[inline]{Gleam ca. 20\% heller}
\todo[inline]{Gamma-korrigierte Umwandlungen von Vorteil}
\todo[inline]{beste Kandidaten nennen}
\todo[inline]{Lightness und wahrnehmungspsychologie völlig egal}

\begin{figure}
	\centering
	\begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{conversion/rgb}
        \caption{\RGB}
        \label{fig:conv_rgb}
    \end{subfigure}
    \quad 
    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{conversion/intensity}
        \caption{Intensity}
        \label{fig:conv_intensity}
    \end{subfigure}

    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{conversion/value}
        \caption{Value (\HSV)}
        \label{fig:conv_value}
    \end{subfigure}
    \quad
    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{conversion/luster}
        \caption{"`Luster"' (\HLS)}
        \label{fig:conv_luster}
    \end{subfigure}

    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{conversion/luminance}
        \caption{Luminance}
        \label{fig:conv_luminance}
    \end{subfigure}
	\quad
    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{conversion/gleam}
        \caption{Gleam}
        \label{fig:conv_gleam}
    \end{subfigure}

    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{conversion/luma}
        \caption{Luma}
        \label{fig:conv_luma}
    \end{subfigure}
    \quad
    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{conversion/lightness}
        \caption{Lightness (\LAB, \LUV)}
        \label{fig:conv_lightness}
    \end{subfigure}

    \caption{Vergleich verschiedener Konvertierungen von Farb- zu Grauwerten}
    \caption*{Vergleich der in \cref{subsec:graustufen} beschriebenen Konvertierungen der \RGB-Farbkanäl von \cref{fig:conv_rgb} in Intensitätswerte.}
\end{figure}

\section{Normalisierung der Intensitäten}

\todo[inline]{Befreiung der Bildwerte von Mittelwert und Normalisierung durch Standardabweichung}
\todo[inline]{Begründung und Quelle}

Der mittlere Intensitätswert $\mean{I}$ eines Bildes $\mat I$ der Dimension $M \times N$ berechnet sich als
%
\begin{align}
\mean{I} &= \frac{1}{M \times N} \sum_y \sum_x \mat I[x, y] \label{eq:mean}
\end{align}

Bei einer naiven Implementierung wird mit einer eine Doppelschleife über das Bild iteriert, die jeweilige Pixelintensität pro Kanal aufsummiert und anschließend mit der Anzahl der Bildwerte normiert.
Hierbei muss bedacht werden, dass ein Überlauf der Summenvariablen stattfinden kann; dass dies  bei geeigneter Wahl der Datentypen unwahrscheinlich ist, zeigt folgende Betrachtung:

Bei einem Bild mit 8 Bit Farbtiefe je Pixel (d.h. Wertebereich $0 \dots 255$ pro Kanal) ist die maximal erreichbare Summe $M \times N \times 255$.
Bei einem Bild mit \SI{4}{\mega\px} (z.B. $\num{2000} \times \SI{2000}{\px}$) entspricht dies dem Wert $\num{4000000} \times 255 = \num{1020000000}$, d.h. ca. 1 Milliarde.

Ein vorzeichenfreier 32-Bit-Datentyp (\datatype{unsigned int}) bildet jedoch bereits den Wertebereich $2^0 \dots 2^{32} = 0 \dots \num{4294967295}$ ab; das heißt, die maximale kumulative Intensität beträgt in diesem Fall ca. 4,3 Milliarden. 
Unter der Annahme einer vollen Aussteuerung (d.h. jedes Pixel desselben Kanals hat die Intensität $255$) ist der Wertebereich für Bilder bis $\sfrac{2^{32}}{255} \approx \SI{16843009}{\mega\px}$ abgedeckt; dies entspricht ungefähr einem Bild der Dimension $\num{4104} \times \SI{4104}{\px}$.
Bei Verwendung eines vorzeichenfreien 64-Bit-Datentyps (\datatype{unsigned long}) beträgt der maximale Wert bereits $2^{64} = \num{18446744073709551616}$, d.h. 18 Trillionen, und entspräche bei gleichen Bedingungen 72 Billiarden Pixeln -- etwa einem Bild der Dimension $\num{268961285} \times \SI{268961285}{\px}$ und einem Speicherbedarf von ca. $\SI{72340172}{\gibi\byte}$ pro Kanal.
Da heutige Prozessorarchitekturen \todo{Beispiele} auf die Verarbeitung von 64-Bit-Typen spezialisiert sind, ist ein solcher eine geeignete Wahl für eine derartige naive Implementierung.

\todo[inline]{Übergang zur Berechnung der (Stichproben-)Varianz (benötigt Erwartungs- oder Mittelwert) und Begründung für zweiphasige Berechnung, da Sobel-Operator.}
\todo[inline]{Beschreibung der on-line-Berechnung von Mittelwert und Standardabweichung}

Die Varianz der Bildintensitäten ist die mittlere quadratische Abweichung der einzelnen Intensitäten von ihrem Mittelwert $\mean{I}$:
%
\begin{align}
\sigma^2_I &= \frac{1}{M \times N} \sum_y \sum_x \left( \mat I[x, y] - \mean{I} \right)^2 \label{eq:variance}
\end{align}

Diese Formel setzt bei naiver Implementierung die Kenntnis des Mittelwertes $\mean{I}$ voraus und bedarf daher einer zusätzlichen Iteration über alle Pixel, wenn dies nicht der Fall ist.
Eine Form der on-line-Berechnung von Varianz und Mittelwert, die in nur eines Durchlaufes über alle Pixel bedarf, beschreibt \cite[216]{knuth85Seminumerical} als rekursive Berechnung über alle Bildwerte $x_k, k \in \left[1, M \times N \right]$ (\cref{eq:onlineVarianceM,eq:onlineVarianceS}), wobei sich die Varianz $\sigma^2_I$ gemäß \cref{eq:onlineVariance} ergibt:
%
\begin{align}
\begin{split}
	M_1 &= x_1 \\
	M_{k} &= M_{k-1} + \frac{(x_k - M_{k-1})}{k}
\end{split} \label{eq:onlineVarianceM} \\ \notag \\
\begin{split}
	S_1 &= 0 \\
	S_{k} &= S_{k-1} \left( x_k - M_{k-1} \right) \left( x_k - M_k \right)
\end{split} \label{eq:onlineVarianceS} \\ \notag \\
\begin{split}
	\mean{x}_n &= M_n \\
	\sigma^2_I &= \sigma^2_n = \frac{S_n}{n - 1} \quad \text{mit} \quad n = M \times N \label{eq:onlineVariance}
\end{split}
\end{align}
%
Nachteilig ist hierbei jedoch die in jedem Durchgang durchzuführende Division, welche die Verwendung von Fließkomma- oder Fixed-Point-Operationen erfordert, die ihrerseits sich aggregierende Rundungsfehler mit sich führen können.

\section{Cropping und Resampling}

\todo[inline]{Da die Ausgangsbilder im Wesentlichen aus (zum Großteil freigestellten) Produkt-Werbefotos bestehen ...}

\begin{figure}
	\centering
\begin{tikzpicture}
    
	\node[inner sep=0pt] (product) at (0,0)
    	{\includegraphics[width=.5\textwidth]{7A5F52750ADD07483305B0C2226A484ED74B42FD4D87B4BBBC352DCF4E8D8BB8}};
    	
	\draw[<->,shorten <= 1pt] (product.north west) -- (product.north east)
	    node[midway,fill=white] {\SI{1200}{\px}};    	
	\draw[<->,shorten <= 1pt] (product.north west) -- (product.south west)
	    node[rotate=90,midway,fill=white] {\SI{1450}{\px}};

	\coordinate (GL) at (product.south west);
	\coordinate (GR) at (product.south east);

	\coordinate (GT) at (product.north east);
	\coordinate (GB) at (product.south east);

	\coordinate (PL) at ($(GL)!115/1200!(GR)$);
	\coordinate (PR) at ($(GL)!1114/1200!(GR)$);

	\coordinate (PT) at ($(GT)!496/1450!(GB)$);
	\coordinate (PB) at ($(GT)!999/1450!(GB)$);

	\draw[<->] (PL) -- (PR)
	    node[midway,fill=white] {\SI{1009}{\px}};

	\draw[<->] (PT) -- (PB)
	    node[rotate=90,midway,fill=white] {\SI{514}{\px}};
	    
	\draw[dotted] (GL) -- (PL);
	\draw[dotted] (PR) -- (GR);
	\draw[dotted] (PB) -- (GR);
	\draw[dotted] (GT) -- (PT);
    	
\end{tikzpicture}
	\caption{Vorverarbeitung: Beispielbild Produktfoto}
	\caption*{Produkt "`Sonnenbrille, Farbverlauf, Leo, Havanna"', Michael Kors, \cite{Conleys:2015:Sonnenbrille:Online}}
	\label{fig:vorverBild}
\end{figure}

Das in \cref{fig:vorverBild} gezeigte Quellbild weist eine Dimension von $\SI{1200}{\px} \times \SI{1450}{\px}$ (Breite $\times$ Höhe) auf, von denen das eigentliche Produktbild jedoch nur ca. $\SI{1009}{\px} \times \SI{514}{\px}$ in Anspruch nimmt.

Würde dieses Bild nun zwecks Verringerung des Speicherbedarfs und der weiteren Verarbeitungszeiten auf eine maximale Kantenlänge von z.B. \SI{512}{\px} heruntergerechnet werden --- dies entspräche $\SI{424}{\px} \times \SI{512}{\px}$ --- so würde das eigentliche Produktbild im resultierenden Format nur noch $\SI{357}{\px} \times \SI{181}{\px}$ beanspruchen.

Durch ein zuvoriges Beschneiden des Bildes auf den relevanten Bildbereich 
-- eine genaue Definition des Begriffes "`relevant"' steht hierbei noch aus -- 
ergäbe sich indes ein Bereich von $\SI{512}{\px} \times \SI{261}{\px}$ für bildwichtige Inhalte.
Diese Vorgehensweise führt in diesem Beispiel folglich zu einer Verdoppelung der nutzbaren Datenmenge gegenüber den naiven Ansatz.
Wird statt der längsten Kante die kürzeste Kante normiert, ist die nutzbare resultierende Bildfläche um so höher, jedoch auch der zur Verarbeitung notwendige Rechenaufwand.
\Cref{tab:vorverDimensionen} verdeutlicht die Dimensionen und prozentualen Unterschiede.

\begin{table}
	\centering
	\begin{tabular}{lrrrr}
		\toprule
		Bild & Breite & Höhe & Anzahl Pixel & Anteil Originalgröße \\
		\midrule
		$G_0$ & \SI{1200}{\px} & \SI{1450}{\px} & \SI{1740000}{\px} & \SI{100}{\percent} \\
		$P_0$ & \SI{1009}{\px} & \SI{514}{\px} & \SI{518626}{\px} & \SI{29.81}{\percent} \\
		\midrule
		$G_1$ & \SI{424}{\px} & \textbf{\SI{512}{\px}} & \SI{217088}{\px} & \SI{12.47}{\percent} \\	
		$P_1$ & \SI{357}{\px} & \SI{181}{\px} & \SI{64617}{\px} & \SI{3.71}{\percent} \\
		\midrule
		$G_2$ & \textbf{\SI{512}{\px}} & \SI{619}{\px} & \SI{316928}{\px} & \SI{18.21}{\percent} \\	
		$P_2$ & \SI{431}{\px} & \SI{219}{\px} & \SI{94389}{\px} & \SI{5.42}{\percent} \\		
		\midrule
		$G_3 = P_3$ & \textbf{\SI{512}{\px}} & \SI{261}{\px} & \SI{133632}{\px} & \SI{7.68}{\percent} \\
		$G_4 = P_4$ & \SI{1005}{\px} & \textbf{\SI{512}{\px}} & \SI{514560}{\px} & \SI{29.57}{\percent} \\
		\bottomrule
	\end{tabular}
	\caption{Vorverarbeitung: Bilddimensionen im Vergleich}
	\caption*{$G_0, P_0$: Bereich des Gesamtbildes $G$ und Produktbildes $P$ in den ursprünglichen Dimensionen; $G_1, P_1$: Bereiche nach Verkleinerung der längsten Kante des Bildes $G$ auf \SI{512}{px}; $G_2, P_2$: Bereiche nach Verkleinerung der kürzesten Kante des Bildes $G$ auf \SI{512}{px}; $G_3, P_3$: Bereiche nach Entfernung der bildunwichtigen Randteile in $G$, gefolgt von Reduktion der längsten Kante auf \SI{512}{\px}; $G_4, P_4$: Analog $G_3, P_3$, jedoch Reduktion der kürzesten Kante auf \SI{512}{\px}.}
	\label{tab:vorverDimensionen}
\end{table}

\section{Energiebasiertes Retargeting}

Bildretargeting befasst sich mit der Aufgabe, ein Bild fester Dimension auf Zielen mit vom Bild unterschiedlicher oder generell variabler Dimension darzustellen, wie es etwa im Rahmen von responsive designs von Webseiten \todo{erklären} oder bei Smartphones beim Wechsel von vertikaler Lage ("`portrait mode"') zu horizontaler Lage ("`landscape mode"') notwendig wird.
%
Ein prominentes, aber nicht mehr all zu oft auftretendes Beispiel für dieses Problem ist die Darstellung von 4:3-Filmformaten auf einem Fernseher mit einem 16:9- oder 16:10-Bildverhältnis, wobei man sich zwischen vollem Bild, aber reduzierter Bildhöhe ("`letterbox"') oder voller Ausnutzung der Höhe der Anzeigefläche, aber fehlenden Bildrändern ("`Pan \& Scan"') entscheiden muss.

Während man einen Text dynamisch entsprechend des zur Darstellung verfügbaren Platzes umbrechen kann, ist dies bei Bildern nicht möglich.
Übliche Optionen in diesen Fällen sind das Beschneiden des Bildes auf die Dimensionen der Anzeigefläche (crop), das Skalieren des Bildes in den neuen Anzeigebereich unter Beachtung des Aspektverhältnisses (zoom, scale to fit), sowie das Skalieren des Bildes ohne Beachtung des Aspektverhältnisses (stretching), wobei jede dieser Optionen ihre eigenen Vor- und Nachteile mit sich bringt.

Ein wünschenswerter Zustand ist es, Bildteile stattdessen entsprechend ihrer Wichtigkeit zu behalten oder zu verwerfen. 
\cite{avidan2007seam} formulieren die "`Wichtigkeit"' eines Pixels als dessen Informationsgehalt, ausgedrückt über die Energie $\fun e_1(\mat I)$ des Bildes $\mat I$:
%
\begin{align}
\fun e_1(\mat I) &= |\frac{\partial}{\partial x} \mat I| + |\frac{\partial}{\partial y} \mat I| \label{eq:seamEnergy1}
\end{align}
%
Hierbei entsprechen die beiden Teilsummen den partiellen Ableitungen des Bildes in $x$- und $y$-Richtung.

\subsection{Eindimensionale Faltung zur approximativen Differenzierung}

Eine Approximation der Ableitung $\sfrac{\partial \fun f}{\partial x}$ ist der zentrale Differenzenquotient
%
\begin{align}
\frac{\partial \fun f}{\partial x} &\approx \Delta f(x) = \frac{\fun f(x+\Delta x) - \fun f(x-\Delta x)}{2\Delta x} \label{eq:centralDifferenceOperator}
\end{align}
%
der sich bei einer diskreten Schrittweite von $\Delta x = \Delta k = 1$ als Faltung (\engl{convolution}) $(\fun f \ast \fun g)[k]$ von $\fun f[k]$ mit der Funktion
\begin{align}
\fun g[k] = 
	\begin{cases}
		-\frac{1}{2} & \text{wenn } k = -1 \\
		+\frac{1}{2} & \text{wenn } k = +1 \\
		0 & \text{sonst}
	\end{cases} \label{eq:conv1DKernel}
\end{align}
%
Die diskrete Faltung ist dabei definiert als:
%
\begin{align}
(\fun f \ast \fun g)[n] &= \sum_k \fun f[k] \cdot \fun g[n - k] \\
              &= \sum_k \fun f[n - k] \cdot \fun g[k] \label{eq:diskFaltung}
\end{align}

Es zeigt sich, dass diese einer gewichteten Summenbildung der Funktion $f$ mit allen Gewichtswerten $g$ entspricht%
\footnote{Der Umkehrung der Betrachtungsrichtung von $k$ zu $-k$ verdient die Faltung ihren Namen, analog einem gefalzten Blatt, dessen eine Hälfte auf die andere gefaltet wird.}.
Die Variation in \cref{eq:diskFaltung} ergibt sich aus der Kommutativität der Faltungsoperation und ist oft besser geeignet, um Gewichtungen mit einer begrenzten Anzahl an Koeffizienten (z.B. einer finiten Impulsantwort) darzustellen%
\footnote{Die Mächtigkeit der Menge der Gewichtungskoeffizienten $g$ ist im Allgemeinen geringer als die Mächtigkeit der Menge der zu gewichtenden Funktionswerte $f$; ein FIR-Filter ist üblicherweise "`kürzer"' als das zu filternde Signal.}.

\Cref{eq:comparisonConv1DWithCentralDiff} zeigt die Identität zwischen der Faltung einer Funktion $f[n]$ mit der in \cref{eq:conv1DKernel} dargestellten Funktion $g[n]$:
%
\begin{align}
\label{eq:comparisonConv1DWithCentralDiff}
\begin{split}
(\fun f \ast \fun g)[n]
                &= \overbrace{\fun f\left[n - \left(-1\right)\right] g[-1]}^{k = -1}
              \,+\, \overbrace{\fun f\left[n - 0\right] g[0]}^{k = 0}
              \,+\, \overbrace{\fun f\left[n - \left(+1\right)\right] g[+1]}^{k = +1} \\
              &= \fun f\left[n + 1\right] \cdot \left(+\frac{1}{2}\right) 
              + \fun f\left[n\right] \cdot (0) 
              + \fun f\left[n - 1\right] \cdot \left(-\frac{1}{2}\right) \\              
              &= \frac{\fun f\left[n + 1\right] - \fun f\left[n - 1\right]}{2}
\end{split}
\end{align}

Bei Darstellung der Funktion $f[n]$ und des Faltungskerns $g[n]$ in vektorieller Form als $\vec f$ und $\mat g$ schreibt sich die Faltung generalisiert auf eine oder zwei Dimensionen als
%
\begin{align}
(\fun f \ast \fun g)(n) = \mat f \ast \vec g
\end{align}
%
Der in \cref{eq:conv1DKernel} beschriebene eindimensionale Faltungskern zur Berechnung des zentralen Differenzenquotienten zur diskreten Approximation des Differentialquotienten gemäß \cref{eq:centralDifferenceOperator} beschreibt sich so als
%
\begin{align}
\mat g_{diff} = \frac{1}{2} \begin{bmatrix}
 -1 & 0 & +1
 \end{bmatrix} \label{eq:len3diffKernel}
\end{align}

Da Differenzierungsverfahren auf abrupte Werteveränderungen entsprechend stark reagieren, sind sie anfällig gegenüber Störprozessen, die das Nutzsignal überlagern.
Dieses generelle Problem der stochastischen%
\footnote{Ist die Überlagerung eines Störprozesses exakt beschreib- und damit korrigierbar, handelt es sich nicht länger um ein stochastisches, sondern deterministisches Signal; in Folge der a priori-Kenntnis des Inhaltes erübrigt sich dann dessen Verarbeitung.}
Signalverarbeitung kann durch ein Glätten des Signales gemindert werden:
Hierbei wird die relative Veränderung eines Wertes zu seinen Nachbarwerten beschränkt, wodurch die Amplitude des Rauschsignales auf Kosten der Amplitude des Nutzsignales gemindert wird.
Eine zugrunde liegende Annahme dürfte sein, dass natürlich vorkommende -- genereller: energetisch günstige -- Folgezustände nicht stark variieren%
\footnote{"`Natura non facit saltus"', Die Natur macht keine Sprünge; Carl von Linné.}, ein Wert also im Allgemeinen ähnlich zu seiner Umgebung ist.

Einen einfachen, aber effektiven Glättungsprozess stellt dabei der (lokal) gewichtete Mittelwert dar, welcher die Intensität eines Wertes -- z.B. Pixels -- aus seiner Nachbarschaft ermittelt.
Zwei eindimensionale Glättungskern in horizontaler Richtung stellen \cref{eq:len3meanKernel1D} (ein einfacher Mittelwert über die direkte Nachbarschaft), sowie \cref{eq:len3smoothKernel1D} (ein gewichteter Mittelwert über dieselbe Nachbarschaft) dar:
%
\begin{align}
\mat g_{mean} = \frac{1}{3} \begin{bmatrix}
 1 & 1 & 1
 \end{bmatrix} \label{eq:len3meanKernel1D} \\
\mat g_{smooth} = \frac{1}{4} \begin{bmatrix}
 1 & 2 & 1
 \end{bmatrix} \label{eq:len3smoothKernel1D}
\end{align}

In jedem Fall kollidiert jedoch die Aufgabe der Differenzierung in Laufrichtung des Filters mit der Glättung in derselben Richtung, da beide Operationen gegenteilige Ziele verfolgen (jener die Detektion, dieser die Minderung von Sprüngen).

\subsection{Sobel-Operator und zweidimensionale Faltung}

Der \algo{Sobel-Operator} (nach \cite{Sobel:1968:Talk}, beschrieben in \cite{Sobel:2015:Online}) löst dieses Problem im zweidimensionalen Raum, indem er die Glättungsoperation stattdessen orthogonal zur Suchrichtung ausführt.
%Soll in horizontaler Richtung differenziert werden, wird hierzu ("`zuerst"') in vertikaler Richtung geglättet.
%
Dabei bedient er sich der zweidimensionalen diskreten Faltung
%
\begin{align}
(\fun f \ast \fun g)[m, n] &= \sum_k \sum_l \fun f[k, l] \cdot \fun g[m - k, n - l] \label{eq:diskFaltung2D}
\end{align}
%
welche einen zweidimensionalen Filterkern (eine Matrix) auf eine zweidimensionale Funktion (ebenfalls eine Matrix) anwendet.

Der 2D-Filterkern des Sobel-Operators zur Differenzierung in horizontaler Richtung $\mat G_{x}$ ergibt sich nun aus der Überlagerung der Kerne $\mat g_{smooth}$ und $\mat g_{diff}$ mittels des dyadischen (bzw. Kronecker-)Produktes als
%
\begin{align}
\mat G_{x} &= \frac{1}{8}
\begin{bmatrix}
 -1 & 0 & 1 \\
 -2 & 0 & 2 \\
 -1 & 0 & 1
 \end{bmatrix} = \mat g_{smooth}^T \otimes \mat g_{diff} \label{eq:sobel3x}
\end{align}

Analog existiert ein Operator $\mat G_{y}$ für Differenzierung in vertikaler Richtung mit
%
\begin{align}
\mat G_{y} &= \frac{1}{8}
\begin{bmatrix}
 -1 & -2 & -1 \\
 0 & 0 & 0 \\
 1 & 2 & 1
 \end{bmatrix} = \mat g_{diff}^T \otimes \mat g_{smooth} \label{eq:sobel3y}
\end{align}

Der Vektor der partiellen Ableitungen $\frac{\partial \mat I}{\partial x}$, $\frac{\partial \mat I}{\partial y}$ beschreibt den Gradienten $\nabla \mat I$ eines Bildes
%
\begin{align}
\nabla \mat I = 
	\begin{bmatrix}
		\sfrac{\partial \mat I}{\partial x} \\
		\sfrac{\partial \mat I}{\partial y}
	\end{bmatrix} \approx
	\begin{bmatrix}
		\Delta_x \mat I \\
		\Delta_y \mat I
	\end{bmatrix}  \label{eq:gradient}
\end{align}
%
welcher anschaulich eine Menge von Richtungsvektoren beschreibt, entlang derer die einzelnen Pixel des Bildes ausgerichtet sind; der Sobel-Operator beschreibt eine mögliche Approximation des "`echten"' Gradienten.

Eine nennenswerte Eigenschaft der Differenzierung von Bildern ist, dass die so ermittelten Intensitätssprünge üblicherweise \todo{Quelle} einhergehen mit dem Vorhandensein von Kanten abgebildeter Objekte, weswegen der Sobel-Operator regelmäßig als einfacher%
\footnote{Der Sobel-Operator ist nicht anisotrop; seine Sensitivität gegenüber diagonalen Kanten -- insbesonderer solcher im 45°-Winkel zu den beiden Suchrichtungen -- ist minimal.}, wenngleich effizienter Kantendetektor eingesetzt wird.
\Cref{fig:sobelGlasses,fig:sobelMascara} demonstrieren die Wirkung des Sobel-Operators.

Analog \cref{eq:seamEnergy1} ergibt sich nun das Energiebild des Bildes $\mat I$ als
%
\begin{align}
\begin{split}
\fun e_1(\mat I) &\approx |\mat G_x \ast \mat I| + |\mat G_y \ast \mat I| \\
                 &= |\Delta_x \mat I| + |\Delta_y \mat I|
\end{split} \label{eq:sobelEnergy}
\end{align}
%
das heißt der absoluten Summe des mit jedem Sobel-Operatoren separat gefalteten Bildes.
\Cref{fig:mascaraEnergy} zeigt exemplarisch das Energiebild von \cref{fig:mascara}.

\todo[inline]{Weitere Energie ergibt sich über Grauwert-Cooccurence-Matrix, siehe \url{http://de.mathworks.com/help/images/ref/graycoprops.html} und 
\url{http://de.mathworks.com/help/images/ref/graycomatrix.html}}
\todo[inline]{Referenz auf Quadrat der Beträge aus Wahrscheinlichkeit des Intensitätswertes -- analog Cooccurence \url{http://stackoverflow.com/a/34273211}}

\subsection{Separierbare Faltungskerne}

Ein zweidimensionales Filter, das wie der Sobel-Operator aus dem dyadischen (bzw. Kronecker-) Produkt $\vec a \otimes \vec b$ zweier Vektoren $\vec a, \vec b$ zusammengesetzt ist, wird separierbar genannt. \todo{Quelle}
\todo[inline]{Jede Matrix des Ranges 1 ist separierbar und kann mittels Singulärwertzerlegung in ihre äußeren Faktoren zerlegt werden. -- Quelle! (Wikipedia ...)}

Da die Faltung nicht nur kommutativ, sondern auch assoziativ ist -- $(\fun f \ast \fun g) \ast \fun h = \fun f \ast (\fun g \ast \fun h)$ -- gilt für separierbare Kerne $\mat G$
%
\begin{align}
\mat G \ast \mat I = (\vec g^T \ast \vec h) \ast \mat I = (\vec g^T \ast \mat I) \ast \vec h
\end{align}
%
Die Faltung der Matrix $\mat I$ mit einem separierbaren Filter $\mat G = \vec g^T \ast \vec h$ ist folglich identisch mit der Faltung der Matrix $\mat I$ mit dem Vektor $\vec g^T$, gefolgt von der Faltung des Ergebnisses mit dem Vektor $\mat h$.

Die Faltung einer $M \times N$-Matrix (z.B. eines Grauwertbildes) mit einem $P \times Q$-Filterkern benötigt bei einfacher Implementierung%
\footnote{Die Faltung entspricht der Multiplikation der fouriertransformierten, weswegen es effizient sein kann, die Faltung vollständig im Fourierraum durchzuführen.}
$M \times N \times P \times Q$ Multiplikationen und Additionen (jeder Wert der Matrix muss mit jedem Wert des Kernes multipliziert werden). 
Unter Ausnutzung der Separierbarkeit kann die Filterung in zwei Schritten zu je $M \times N \times P$ bzw. $M \times N \times Q$ Operationen durchgeführt werden; in Summe werden hierfür demnach $M N P + M N Q = M N (P + Q)$ Operationen benötigt.
Separierbare Filter können folglich um Faktor $\sfrac{P Q}{(P + Q)}$ schneller angewandt werden:
Für einen $3 \times 3$-Filterkern ergibt sich eine Beschleunigung um Faktor $1.5$, für $5 \times 5$-Filterkerne bereits ein Speedup von $2.5$.

\begin{figure}
	\centering
	\begin{subfigure}[b]{0.8\textwidth}
        \includegraphics[width=\textwidth]{7A5F52750ADD07483305B0C2226A484ED74B42FD4D87B4BBBC352DCF4E8D8BB8-sobelX}
        \caption{Ausschnitt gefiltert mit horizontalem Sobel-Operator, \cref{eq:sobel3x}}
        \label{fig:sobelX}
    \end{subfigure}
    
    \begin{subfigure}[b]{0.8\textwidth}
        \includegraphics[width=\textwidth]{7A5F52750ADD07483305B0C2226A484ED74B42FD4D87B4BBBC352DCF4E8D8BB8-sobelY}
        \caption{Ausschnitt gefiltert mit vertikalem Sobel-Operator, \cref{eq:sobel3y}}
        \label{fig:sobelY}
    \end{subfigure}

    \caption{\Cref{fig:vorverBild} gefiltert mit Sobel-Operator}
    \caption*{Nur der zentrale Bildausschnitt wird dargestellt. Eine Skalierung um $\sfrac{1}{8}$ und Offsetverschiebung um $+127$ zurück in den Wertebereich $0 \dots 255$ ($0$ entspricht schwarz, $255$ entspricht weiß) wurde für Darstellungszwecke vorgenommen; Neutralgrau (Helligkeitswert $127$) entspricht Kantenfreiheit, hellere Töne einer steigenden Flanke (Suchrichtung links nach rechts bzw. oben nach unten), dunklere Töne einer fallenden Flanke.}
    \label{fig:sobelGlasses}
\end{figure}

\begin{figure}
	\centering
	\begin{subfigure}[b]{0.25\textwidth}
        \includegraphics[width=\textwidth]{E559256E6CEA31B79B411104084DF35F9370B147FA3E9BF668FD2FE07367702D}
        \caption{Original} \label{fig:mascara}
    \end{subfigure}
    %
    \begin{subfigure}[b]{0.25\textwidth}
        \includegraphics[width=\textwidth]{E559256E6CEA31B79B411104084DF35F9370B147FA3E9BF668FD2FE07367702D-intensity}
        \caption{Intensity} \label{fig:mascara_intensity}
    \end{subfigure}
    %
    \begin{subfigure}[b]{0.25\textwidth}
        \includegraphics[width=\textwidth]{E559256E6CEA31B79B411104084DF35F9370B147FA3E9BF668FD2FE07367702D-gleam}
        \caption{Gleam} \label{fig:mascara_gleam}
    \end{subfigure}
           
    \caption{Graustufenumwandlung Intensity und Gleam}
    \caption*{Umwandlung des Originalbildes mittels Intensity- und Gleam-Methode. \\ \centering Mascara, \cite{Lacome:2015:Mascara:Online}}
\end{figure}

\begin{figure}
	\centering
    \begin{subfigure}[b]{0.25\textwidth}
        \includegraphics[width=\textwidth]{E559256E6CEA31B79B411104084DF35F9370B147FA3E9BF668FD2FE07367702D-sobelX}
        \caption{$\Delta_x \mat I$}
        \label{fig:mascaraSobelX}
    \end{subfigure}
    \quad
    \begin{subfigure}[b]{0.25\textwidth}
        \includegraphics[width=\textwidth]{E559256E6CEA31B79B411104084DF35F9370B147FA3E9BF668FD2FE07367702D-sobelY}
        \caption{$\Delta_y \mat I$}
        \label{fig:mascaraSobelY}
    \end{subfigure}    
    \quad
    \begin{subfigure}[b]{0.25\textwidth}
        \includegraphics[width=\textwidth]{E559256E6CEA31B79B411104084DF35F9370B147FA3E9BF668FD2FE07367702D-energy}
        \caption{$|\Delta_x \mat I| + |\Delta_y \mat I|$}
        \label{fig:mascaraEnergy}
    \end{subfigure}     
           
    \caption{\Cref{fig:mascara} gefiltert mit Sobel-Operator, Energiebild}
    \caption*{Anwendung der Sobel-Operatoren aus \cref{eq:sobel3x,eq:sobel3y} in X- und Y-Richtung auf \cref{fig:mascara}, sowie Energiebild gemäß \cref{eq:sobelEnergy}.
    Normierung auf Wertebereich $0 \dots 255$ analog \cref{fig:sobelGlasses}.}
    \label{fig:sobelMascara}
\end{figure}

\todo[inline]{Stellt Kanten mit doppelter Breite dar}
\todo[inline]{Darstellung als Stems und Pixelwerte!}
\todo[inline]{Sinnvolle Begründung für zentralen Differenzenquotienten}
\todo[inline]{Intuitive Erklärung für zentralen Differenzenquotienten}
\todo[inline]{Sample und Plots für gefilterte Daten}

\chapter{Übersicht der Literatur}

\cite{orb} beschreiben einen neuen binären Deskriptor genannt \algo{ORB}, der auf \algo{BRIEF} basiert, jedoch um Rotationsinvarianz und Rauschresistenz erweitert wurde. \algo{ORB} ist (bei vergleichbarer Performance) um zwei Magnituden schneller als \acro{SIFT}. Unterstellt dabei, dass \algo{SURF} besser ist als \algo{SIFT}; liefert Quelle für \algo{SURF}.

\cite{eliveindonesia} ist eine Präsentation zum Entwurf eines Nutz"-vieh-Klas"-si"-fi"-ka"-tions"-sys"-tems, das Ansätze zur Parallelisierung von \algo{SIFT} und \algo{SURF} anmerkt. Hierbei werden unter anderem Parallelisierungen der Algorithmen (\algo{Parallel SIFT} und \algo{Parallel SURF}) besprochen, sowie auf Mehrmaschinen-Verarbeitung der Bilder selbst angeht; Hintergrund ist, dass die Features selbst lokaler Natur sind, für die Auswertung also nicht unbedingt der gesamte Kontext des Bildes notwendig ist. Eine Einführung von Padding an den Bildrändern wird ebenfalls erwähnt.

\cite{detmatchkeyproadPresi} vergleichen \algo{SIFT} mit \algo{Affine-SIFT} und anderen (Stichworte \algo{FLANN} und \algo{RANSAC} -- Hinweis: \algo{FLANN} ist auch als Functional Link Neural Network bekannt, meint hier jedoch Fast Linear Approximation of Nearest Neighbors) und weist darauf hin, dass \algo{SIFT} nur in vier Parametern invariant ist: Zoom, Rotation, sowie X- und Y-Translation; affine Transformationen wie Perspektivenänderungen oder Rotation um eine geneigte Kameraachse sind dabei inbegriffen (d.h. gegenüber Kameratilts ist Standard-\algo{SIFT} nicht invariant).

Hinweis auf den "`Curse of dimensionality"' (den "`Fluch der Dimensionalität"'), 
siehe \url{https://en.wikipedia.org/wiki/Curse_of_dimensionality}, zur Begründung,
weswegen Kd-Trees prinzipiell ungeeignet für das Indexieren von \algo{SIFT}-Features ist (diese sind sehr hochdimensional).

\cite{zha2010computer} beschreiben als \cite{zha2010computer_mift} (siehe \url{http://link.springer.com/chapter/10.1007/978-3-642-12304-7_50}) den \algo{MIFT}-Algorithmus "`A Mirror Reflection Invariant Feature Descriptor"'.

Zusätzliche Entwicklung mit \algo{MI-SIFT}.

\cite{alhwarin2011fast} nennt drei neue \algo{SIFT}-Ansätze für schnelleres und robusteres Matching in Robotik-Anwendungen.

\cite{yin2013content} beschreiben Content-Based Image Retrieval mittels Apache Hadoop auf Basis von \algo{SURF}-Deskriptoren und "`Locality-Sensitive Hashing"'.

\cite{lowe2004distinctive} ist das Original-Paper von Lowe zu \algo{SIFT}. Er merkt an, dass verschiedene Detektoren wie z.B. der Harris-Kantendetektor stark auf Bildskalierungen reagieren und demnach ungeeignet sind.

\cite{ginesu2012objective} vergleichen \algo{WebP} mit bestehenden Bildformaten wie \algo{JPEG} und \algo{PNG}.

\cite[395]{corke2011robotics} beschreibt \algo{RANSAC} in MATLAB.
\cite[373]{corke2011robotics} beschreibt Featureextraktion.

\chapter{Leerer Abschnitt}

\clearpage
\pagenumbering{Roman}
\appendix
\backmatter

\clearpage
\phantomsection
\addcontentsline{toc}{section}{Bildquellen}
\printbibliography[keyword=image,title={Bildquellen}]

\clearpage
\phantomsection
\addcontentsline{toc}{section}{Literatur}
\printbibliography[notkeyword=image]

\end{document}